<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-12-18T10:20:26+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Cloud Native Journey</title><subtitle>Software Engineer/Architect</subtitle><author><name>Jaeguk Yun</name></author><entry><title type="html">Airflow Sensor</title><link href="http://localhost:4000/workflow/Airflow-Sensor/" rel="alternate" type="text/html" title="Airflow Sensor" /><published>2022-12-18T00:00:00+09:00</published><updated>2022-12-18T00:00:00+09:00</updated><id>http://localhost:4000/workflow/Airflow%20Sensor</id><content type="html" xml:base="http://localhost:4000/workflow/Airflow-Sensor/"><![CDATA[<h2 id="airflow-sensor">Airflow Sensor</h2>
<p>센서는 정확히 한 가지 일을 하도록 설계된 특별한 유형의 오퍼레이터입니다  - 무언가가 발생할 때까지 기다립니다. 시간 기반이거나 파일 또는 외부 이벤트를 기다리는 것일 수 있지만 어떤 일이 발생할 때까지 기다렸다가 해당조건을 만족하면  다운스트림 작업(이후 Task)을 실행할 수 있습니다.</p>

<p><a href="https://airflow.apache.org/docs/apache-airflow/2.2.3/_api/airflow/sensors/index.html">https://airflow.apache.org/docs/apache-airflow/2.2.3/_api/airflow/sensors/index.html</a></p>

<p>Sensor Task는 주기적으로 체크하면 다음 단계로 진행하지 못하고 대기모드로 유지되기 때문에 Airflow DAG에서의 Sensor는 Worker의 슬롯 한 개를 점유합니다. Sensor는 BaseSensorOperator를 상속하여 구현합니다. BaseSensorOperator는 다음의 옵션을 지원합니다.</p>

<table>
  <thead>
    <tr>
      <th>구분</th>
      <th>타입</th>
      <th>기본값</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>poke_interval</td>
      <td>float</td>
      <td>60</td>
      <td>조건 확인을 위한 재시도 주기이며 단위는 second</td>
    </tr>
    <tr>
      <td>timeout</td>
      <td>float</td>
      <td>60 * 60 * 24 * 7</td>
      <td>Sesnor의 조건 확인을 위해 대기하는 시간이며 단위는 Second</td>
    </tr>
    <tr>
      <td>mode</td>
      <td>str</td>
      <td>“poke”</td>
      <td>poke는 특정 조건을 만족할 때까지 worker 슬롯 점유 <br /> reschedule은 조건을 확인할때만 worker 슬롯 점유</td>
    </tr>
  </tbody>
</table>

<p>Sensor의 유형 중에 대표적인 Sensor는</p>

<ul>
  <li>FileSensor</li>
</ul>

<p>이며 그외 다음과 같은 Sensor들이 있습니다.</p>

<table>
  <thead>
    <tr>
      <th>구분</th>
      <th>모듈 경로</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BaseSensor</td>
      <td>airflow.sensors.bash</td>
      <td>조건 확인의 재시도 주기이며 단위는 Second</td>
    </tr>
    <tr>
      <td>DatetimeSensor<br />DateTimeSensorAsync</td>
      <td>airflow.sensors.date_time</td>
      <td>Sensor의 조건 확인 대기 시간이며 단위 Second</td>
    </tr>
    <tr>
      <td>ExternalTaskSensor</td>
      <td>airflow.sensors.external_task</td>
      <td>다른 DAG의 Task가 종료까지 대기하면서 모니터링하는 DAG의 external Task가 종료되면 ExternalTaskSensor가 실행</td>
    </tr>
  </tbody>
</table>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[Airflow Sensor 센서는 정확히 한 가지 일을 하도록 설계된 특별한 유형의 오퍼레이터입니다 - 무언가가 발생할 때까지 기다립니다. 시간 기반이거나 파일 또는 외부 이벤트를 기다리는 것일 수 있지만 어떤 일이 발생할 때까지 기다렸다가 해당조건을 만족하면 다운스트림 작업(이후 Task)을 실행할 수 있습니다.]]></summary></entry><entry><title type="html">Airflow Operators</title><link href="http://localhost:4000/workflow/Airflow-operators/" rel="alternate" type="text/html" title="Airflow Operators" /><published>2022-12-17T00:00:00+09:00</published><updated>2022-12-17T00:00:00+09:00</updated><id>http://localhost:4000/workflow/Airflow%20operators</id><content type="html" xml:base="http://localhost:4000/workflow/Airflow-operators/"><![CDATA[<h2 id="airflow-operators">Airflow Operators</h2>
<p>DAG을 구성하는 작업을 Task라고 하며, DAG이 수행할 작업을 의미합니다. 한개 이상의 Task를 pipeline으로 연결해서 하나의 DAG을 완성해야 합니다.
Task에는</p>

<ul>
  <li>Operator</li>
  <li>Sensor</li>
  <li>Hook</li>
</ul>

<p>가 있습니다.<br />
Operator에서는 대표적인. Bash, Python, Empty 또는 이전버전 Dummy Operator가 있습니다. 
상세한 Operator 정보는 다음의 <a href="https://airflow.apache.org/docs/apache-airflow/2.2.3/operators-and-hooks-ref.html">링크</a>를 참고하세요</p>

<p><a href="https://airflow.apache.org/docs/apache-airflow/2.2.3/operators-and-hooks-ref.html">https://airflow.apache.org/docs/apache-airflow/2.2.3/operators-and-hooks-ref.html</a></p>

<p>Operator는</p>

<ul>
  <li>Action Operator</li>
  <li>Transfer Operator</li>
  <li>Sensor Operator</li>
</ul>

<p>로 구분됩니다. <br />
Action Operator 는 작업을 수행하거나 다른 작업을 수행하도록 trigger합니다.<br />
Transfer Operator는 특정 시스템에 다른 시스템으로 데이터를 이동합니다.<br />
Sensor Operator는 특정 조건에 일치할 때 까지 기다렸다가, 만족되면 이후 과정을 진행하도록 기다려는 Operator.</p>

<p>여기에서는 대표적인 Operator 를 알아보겠습니다.</p>
<ul>
  <li>EmptyOperator</li>
  <li>BashOperator</li>
  <li>PythonOperator</li>
</ul>

<p>기외 주요 Operator는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>구분</th>
      <th>클래스 경로</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BranchPythonOperator</td>
      <td>airflow.operators.branch</td>
      <td>파이션 실행결과에 따른 분기를 설정하는 Operator</td>
    </tr>
    <tr>
      <td>TriggerDagRunOperator</td>
      <td>airflow.operators.trigger_dagrun</td>
      <td>지정한 dag을 실행</td>
    </tr>
    <tr>
      <td>ShortCircuitOperator</td>
      <td>airflow.operators.python</td>
      <td>bool 조건에 맞을 때만 실행 <br /> bool 연산 로직은 python_callable로 전달</td>
    </tr>
    <tr>
      <td>EmailOperator</td>
      <td>airflow.operators.email</td>
      <td>이메일 전송</td>
    </tr>
  </tbody>
</table>

<p>그외 operator들은 다음의 <a href="https://airflow.apache.org/docs/apache-airflow/2.2.3/_api/airflow/operators/index.html">링크</a> 참고합니다.</p>

<p><a href="https://airflow.apache.org/docs/apache-airflow/2.2.3/_api/airflow/operators/index.html">https://airflow.apache.org/docs/apache-airflow/2.2.3/_api/airflow/operators/index.html</a></p>

<p>Bash Operator 예제</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>  
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.empty</span> <span class="kn">import</span> <span class="n">EmptyOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span> 

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span> <span class="o">=</span> <span class="s">'bash-op'</span><span class="p">,</span>
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span><span class="p">,</span>
  <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="s">'training'</span><span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span> 
  <span class="n">start</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span>
    <span class="n">task_id</span> <span class="o">=</span> <span class="s">'start'</span>
  <span class="p">)</span>
  
  <span class="n">end</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span>
    <span class="n">task_id</span> <span class="o">=</span> <span class="s">'end'</span>
  <span class="p">)</span>
  
  <span class="n">bash_task</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">"test_bash"</span><span class="p">,</span>
    <span class="n">bash_command</span> <span class="o">=</span> <span class="s">"echo 'This is the ds: </span><span class="se">\'</span><span class="s">$msg</span><span class="se">\'</span><span class="s">'"</span><span class="p">,</span>
    <span class="n">env</span> <span class="o">=</span> <span class="p">{</span> <span class="s">"msg"</span><span class="p">:</span> <span class="s">''</span><span class="p">}</span>
  <span class="p">)</span>
  
  <span class="n">start</span> <span class="o">&gt;&gt;</span> <span class="n">bash_task</span> <span class="o">&gt;&gt;</span> <span class="n">end</span> 
</code></pre></div></div>

<p>Python Operator 예제</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span>  <span class="n">PythonOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.empty</span> <span class="kn">import</span> <span class="n">EmptyOperator</span>

<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span> 

<span class="kn">import</span> <span class="nn">time</span> 

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'tags'</span><span class="p">:</span> <span class="p">[</span><span class="s">'training'</span><span class="p">],</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">_sleep_func</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Start sleep {sleep_time} seconds'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sleep_time</span> <span class="o">=</span> <span class="n">sleep_time</span><span class="p">))</span>
  <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span> 

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span> <span class="o">=</span> <span class="s">'python-op'</span><span class="p">,</span>
  <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">,</span>
  <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="s">'training'</span><span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span> <span class="p">:</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span> <span class="o">=</span> <span class="s">'start_task'</span><span class="p">)</span>
  <span class="n">end</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span> <span class="o">=</span> <span class="s">'end_task'</span><span class="p">)</span>
  
  <span class="n">task1</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span> <span class="o">=</span> <span class="s">'python_callable_task1'</span><span class="p">,</span>
    <span class="n">python_callable</span> <span class="o">=</span> <span class="n">_sleep_func</span><span class="p">,</span>
    <span class="n">op_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"sleep_time"</span> <span class="p">:</span> <span class="mi">10</span><span class="p">}</span>
  <span class="p">)</span>
  
  <span class="n">start</span> <span class="o">&gt;&gt;</span> <span class="n">task1</span> <span class="o">&gt;&gt;</span> <span class="n">end</span>  
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[Airflow Operators DAG을 구성하는 작업을 Task라고 하며, DAG이 수행할 작업을 의미합니다. 한개 이상의 Task를 pipeline으로 연결해서 하나의 DAG을 완성해야 합니다. Task에는]]></summary></entry><entry><title type="html">Airflow Hooks</title><link href="http://localhost:4000/workflow/Airflow-hooks/" rel="alternate" type="text/html" title="Airflow Hooks" /><published>2022-12-17T00:00:00+09:00</published><updated>2022-12-17T00:00:00+09:00</updated><id>http://localhost:4000/workflow/Airflow%20hooks</id><content type="html" xml:base="http://localhost:4000/workflow/Airflow-hooks/"><![CDATA[<h2 id="airflow-hooks">Airflow Hooks</h2>
<p>Hook은 DB나 서비스 같은 외부 시스템(Database, Storage)과 통신하기 위한 인터페이스를 제공하여 연결상태를 유지하여 작업을 처리하기 위해 사용합니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/09-hooks-component-arch.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>Apache Airflow의 Hook에서csv to db를 또는 db to csv 작업을 할 때 대표적인 Hook은 다음과 같은 것이 있습니다.</p>

<ul>
  <li>PostgresHook</li>
  <li>MySqlHook</li>
  <li>S3</li>
  <li>HDFS</li>
</ul>

<h2 id="apache-airflow-hooks를-실행하는-방법">Apache Airflow Hooks를 실행하는 방법?</h2>

<p>Airflow Hook을 다음의 4단계로 작성합니다 .</p>

<ul>
  <li>Prepare your PostgreSQL Environment</li>
  <li>Create a CSV file using the format</li>
  <li>PostgreSQL 연결 설정 정보 작성</li>
  <li>Airflow PostgresHook DAG 작성</li>
</ul>

<p>Airflow가 제공 한 PostgreSQL Hooks를 사용하여 테이블의 내용을 CSV 파일로 추출합니다.</p>

<p>우선 PostgreSQL에서 테이블을 만들고 일부 데이터를 로드 합니다. 이렇게 하려면 PSQL 쉘로 가서 아래 명령을 실행합니다.</p>

<p>그리고 PostgresHook을 처리하는 DAG을 작성합니다.</p>

<h4 id="prepare-your-postgresql-environment">Prepare your PostgreSQL Environment</h4>

<p>우선 PostgreSQL에서 테이블을 만들고 일부 데이터를로드 해야합니다. 이렇게 하려면 PSQL 쉘에서  아래 명령을 실행합니다.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">customer</span><span class="p">(</span>
  <span class="n">id</span> <span class="nb">serial</span><span class="p">,</span>
  <span class="n">first_name</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
  <span class="n">last_name</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
  <span class="n">email</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="p">);</span>
</code></pre></div></div>

<p>여기에서는 4 개의 열 ID, First_Name, Last_name 및 email 이 있는 고객 테이블을 작성하고 있습니다</p>

<h4 id="create-a-csv-file-using-the-format">Create a CSV file using the format</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>serial,first_name,last_name,email
1,john,michael,john@gmail.com
2,mary,cooper,mcooper@gmail.com
3,sheldon,cooper,scopper@gmail.com
4,john,michael,john@gmail.com
5,mary,cooper,mcooper@gmail.com
6,sheldon,cooper,scopper@gmail.com
</code></pre></div></div>

<p>COPY CSV to POD into customer table</p>

<p>Kubernetes 환경인 경우 다음과 같이 customer.csv파일을 복사합니다.
POD에 customer.csv 파일을 복사합니다.</p>

<ul>
  <li>POSTGRES_POD=$(kubectl get pods -o jsonpath=’{.items[0].metadata.name}’)</li>
  <li>kubectl cp customer.csv $POSTGRES_POD:/tmp</li>
  <li>kubectl exec -it $POSTGRESPOD – bash</li>
</ul>

<p>postgres에 접속합니다.</p>

<p>psql -U postgres</p>

<p>COPY customer FROM ‘/tmp/customer.csv’ DELIMITER ‘,’ CSV HEADER;</p>

<h4 id="postgresql-연결-설정-정보-작성">PostgreSQL 연결 설정 정보 작성</h4>

<p>Airflow UI에서 Admin &gt; Connections 메뉴를 선택하고, Connection Type이 Postgres를 선택하고 Postgres 접속정보를 입력하고 저장합니다.</p>

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Connection Type :</td>
      <td>Postgres</td>
    </tr>
    <tr>
      <td>Host</td>
      <td>ip address</td>
    </tr>
    <tr>
      <td>Schema</td>
      <td>database</td>
    </tr>
    <tr>
      <td>Login</td>
      <td>postgres account</td>
    </tr>
    <tr>
      <td>Password</td>
      <td>postgres password</td>
    </tr>
    <tr>
      <td>Port</td>
      <td>postgres port</td>
    </tr>
  </tbody>
</table>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/09-hooks-postgres-conn.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="airflow-postgreshook-dag-작성">Airflow PostgresHook DAG 작성</h4>

<p>PostgresHook을 다음과 같이 구현하고 테스트합니다</p>

<p>성공적으로 실행되면 파일 저장을 위해 구성된 디렉토리로 이동하면 출력 CSV 파일을 찾을 수 있습니다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.empty</span> <span class="kn">import</span> <span class="n">EmptyOperator</span>
<span class="kn">from</span> <span class="nn">airflow.providers.postgres.hooks.postgres</span> <span class="kn">import</span> <span class="n">PostgresHook</span> 

<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span> 

<span class="kn">import</span> <span class="nn">logging</span> 

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>

<span class="n">POSTGRES_CONN_ID</span> <span class="o">=</span><span class="s">'postgres-conn'</span>

<span class="k">def</span> <span class="nf">export_db_to_csv</span><span class="p">(</span><span class="n">sql</span><span class="p">):</span>
  <span class="n">pg_hook</span> <span class="o">=</span> <span class="n">PostgresHook</span><span class="p">.</span><span class="n">get_hook</span><span class="p">(</span><span class="n">POSTGRES_CONN_ID</span><span class="p">)</span>
  <span class="n">logging</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">'Exporting query to file:{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sql</span><span class="p">))</span>
  <span class="n">pg_hook</span><span class="p">.</span><span class="n">copy_expert</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s">'/opt/airflow/data/customer.csv'</span><span class="p">)</span>
  
<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span> <span class="o">=</span> <span class="s">'postgres-hook-db-to-csv'</span><span class="p">,</span>
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span><span class="p">,</span>
  <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">'training'</span><span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span> 
  <span class="n">start</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">'start'</span><span class="p">)</span>
  <span class="n">end</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">'end'</span><span class="p">)</span> 
  <span class="n">export_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span> <span class="o">=</span> <span class="s">'export-task'</span><span class="p">,</span>
    <span class="n">python_callable</span><span class="o">=</span><span class="n">export_db_to_csv</span><span class="p">,</span>
    <span class="n">op_kwargs</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s">'sql'</span><span class="p">:</span> <span class="s">"COPY (SELECT * FROM CUSTOMER WHERE first_name = 'john' ) TO STDOUT WITH CSV HEADER"</span>
    <span class="p">}</span>
  <span class="p">)</span>
  
  <span class="n">start</span> <span class="o">&gt;&gt;</span> <span class="n">export_task</span> <span class="o">&gt;&gt;</span> <span class="n">end</span> 
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[Airflow Hooks Hook은 DB나 서비스 같은 외부 시스템(Database, Storage)과 통신하기 위한 인터페이스를 제공하여 연결상태를 유지하여 작업을 처리하기 위해 사용합니다.]]></summary></entry><entry><title type="html">Airflow Trigger_rules</title><link href="http://localhost:4000/workflow/trigger_rules/" rel="alternate" type="text/html" title="Airflow Trigger_rules" /><published>2022-12-15T00:00:00+09:00</published><updated>2022-12-15T00:00:00+09:00</updated><id>http://localhost:4000/workflow/trigger_rules</id><content type="html" xml:base="http://localhost:4000/workflow/trigger_rules/"><![CDATA[<h2 id="airflow-trigger-rules">Airflow Trigger rules</h2>
<p>일반적으로 Task는 이전 Task들이 성공할 때만 실행됩니다. 
trigger rule이 default로 all_success이기 때문입니다. 기본적으로 모든 상위 작업이 성공하면 작업이 실행됩니다. 이 action은 일반적으로 기대하는 것입니다.</p>

<p>그러나 더 복잡한 것을 원한다면 어떻게 해야 할까요?
상위 task 중 한 개 task가 성공하자마자 작업을 수행하고 싶다면 어떻게? 
아니면 작업이 실패하면 다른 작업 세트를 실행하고 싶습니까?
또는 작업이 성공하거나 실패하거나 이벤트가 건너 뛸 경우에 따라 다르게 action 을 해야 하는 경우?</p>

<p>좀더 복잡한 workflow는 Task간 다양한 의존성이 존재합니다. 일반적인 워크플로 동작은  모든 직접 업스트림 작업이 성공할 때 작업을 트리거하는 것이지만 Airflow는 더 복잡한 종속성 설정을 허용합니다.
이러한 다양한 의존성을 지원하기 위한 Trigger Rule들이 다음과 같이 존재합니다</p>

<p>모든 Operator 에는  생성된 작업이 트리거되는 규칙을 정의하는 trigger_rule 인수가 있습니다. trigger_rule  의 기본값은 all_success이며 “모든 직접 업스트림 작업이 성공하면 이 작업 트리거”로 정의할 수 있습니다. 여기에 설명된 다른 모든 규칙은 직접 상위 작업을 기반으로 하며 작업을 만드는 동안 모든위 Operator에게 전달할 수 있는 값입니다.</p>

<ul>
  <li>all_success: (default) 모든 상위 Task가 성공한 경우</li>
  <li>all_failed: 모든 Parent Task가 실패 또는 upstream_failed 상태일 떄 하위 Task가 실행</li>
  <li>all_done: 모든 상위 Task가 완료된 경우 하위 Task 실행.</li>
  <li>one_failed: 적어도 한 부모가 실패하자마자 모든 부모가 완료 될 때까지 기다리지 않고 실행됩니다.</li>
  <li>one_success: 적어도 한 부모가 성공하자마자 모든 부모가 완료 될 때까지 기다리지 않고 Trigger 됩니다.</li>
  <li>none_failed: 모든 상위 Task가 실패가 없는 경우(failed or upstream_failed) i.e. 모든 부모가 성공했거나 skip인 경우 하위 Task 실행.</li>
  <li>none_skipped:상위 Task의 상태가 Skip이 없는 경우 하위 Task 실행, i.e. all parents are in a success, failed, or upstream_failed state</li>
</ul>

<p>아래의 task Graph view 처럼 trigger rule이 all_success인 경우 end task는 실행되지 않고 skip하게 되는 경우도 있습니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-none_ailed_min_one_success.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>어떤 dag은 이전 Task에 실패가 없고 최소한 한개 이상 성공한 경우end task를 항상 실행하고자 할 때 end task에 trigger rule을 none_failed_min_one_success
로 설정하면 다음과 같이 end task를 실행할 수 있습니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-none_ailed_min_one_success.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>예시)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="err">’</span><span class="n">end</span><span class="err">’</span><span class="p">,</span> <span class="n">trigger_rule</span><span class="o">=</span><span class="err">’</span> <span class="n">none_failed_min_one_success</span><span class="err">’</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="all_success">all_success</h4>
<p>이것은 매우 간단하고 이미 보았습니다. 모든 업스트림 작업 (부모)이 성공했을 때 작업이 시작됩니다</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-all_success2.png" alt="" />
  <figcaption></figcaption>
</figure>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-all_success3.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="all_failed">all_failed</h4>
<p>모든 상위 작업이 실패하면 Task C 는 작업이 Trigger 됩니다</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-all_failed.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="all_done">all_done</h4>

<p>모든 업스트림 작업 (상위)이 자신의 상태에 관계없이 실행을 수행하면 작업을 트리거하고 합니다. 이 트리거 규칙은 업스트림 작업의 상태에 관계없이 항상 실행하려는 작업이 있는 경우 유용 할 수 있습니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-all_done.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="none_failed">none_failed</h4>

<p>모든 업스트림 작업이 성공하거나 skip이면  Task D는 트리거 됩니다</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-none_failed.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="one_success">one_success</h4>

<p>한 명의 상위 또는 업스트림 작업이 성공하자마자 트리거됩니다. 단 모든 상위 task가 종료될 때까지 기다리지 않습니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-one_success.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="one_failed">one_failed</h4>

<p>한 개의 상위 또는 업스트림 작업이 최소한 1개라도 실패하면 task D는 trigger 됩니다.단 모든 상위 task가 종료될 때까지 기다리지 않습니다</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-one_failed.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="none_failed_min_one_success">none_failed_min_one_success</h4>

<p>한 개의 상위 또는 업스트림 작업이 모두 실패가 없고 최소한 1개라도 성공하면 task D는 trigger 됩니다.단 모든 상위 task가 종료될 때까지 기다리지 않습니다</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/08-trigger-rule-none_failed_min_one_success.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>예제 )</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.empty</span> <span class="kn">import</span> <span class="n">EmptyOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">BranchPythonOperator</span> 
<span class="c1"># Utils 
</span><span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span> 
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">check_condition</span><span class="p">():</span>
  <span class="n">num</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">6</span><span class="p">:</span>
    <span class="k">return</span> <span class="s">'greater'</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="s">'smaller'</span>
  
<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span> <span class="o">=</span> <span class="s">'trigger_rule'</span><span class="p">,</span>
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span><span class="p">,</span>
  <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="s">'training'</span><span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span> 
  <span class="n">start</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span> <span class="o">=</span> <span class="s">'start'</span><span class="p">)</span>
  <span class="n">end</span> <span class="o">=</span> <span class="n">EmptyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">'end'</span><span class="p">,</span> <span class="n">trigger_rule</span> <span class="o">=</span> <span class="s">'all_done'</span><span class="p">)</span>

  
  <span class="n">greater</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span><span class="n">task_id</span> <span class="o">=</span> <span class="s">'greater'</span><span class="p">,</span> <span class="n">bash_command</span><span class="o">=</span><span class="s">'echo "value is greater than 6" &amp;&amp; sleep 30'</span><span class="p">)</span>
  <span class="n">smaller</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span><span class="n">task_id</span> <span class="o">=</span> <span class="s">'smaller'</span><span class="p">,</span> <span class="n">bash_command</span> <span class="o">=</span><span class="s">'echo "value is smaller thant 6" &amp;&amp; exit 1'</span><span class="p">)</span>

  <span class="n">start</span> <span class="o">&gt;&gt;</span> <span class="p">[</span><span class="n">greater</span><span class="p">,</span> <span class="n">smaller</span><span class="p">]</span> <span class="o">&gt;&gt;</span> <span class="n">end</span> 
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[Airflow Trigger rules 일반적으로 Task는 이전 Task들이 성공할 때만 실행됩니다. trigger rule이 default로 all_success이기 때문입니다. 기본적으로 모든 상위 작업이 성공하면 작업이 실행됩니다. 이 action은 일반적으로 기대하는 것입니다.]]></summary></entry><entry><title type="html">Airflow Best Practices - I</title><link href="http://localhost:4000/workflow/airflow-best-practices-1/" rel="alternate" type="text/html" title="Airflow Best Practices - I" /><published>2022-12-14T00:00:00+09:00</published><updated>2022-12-14T00:00:00+09:00</updated><id>http://localhost:4000/workflow/airflow%20best-practices-1</id><content type="html" xml:base="http://localhost:4000/workflow/airflow-best-practices-1/"><![CDATA[<h2 id="좀더-낳은-dag-작성">좀더 낳은 DAG 작성</h2>
<p>DAG는 데이터 파이프라인에 해당합니다. 
DAG는 매일 사용되므로 모범 사례를 따르는 것이 중요합니다.
최적화되고, 이해하기 쉽고, 문서화되고, 잘 조직되어야 합니다. 
수백 개의 DAG로 빠르게 끝날 수 있으므로이 부분을 과소 평가하지 마십시오. 
그것은 당신에게 많은 고통과 문제를 덜어 줄 것입니다.</p>

<h4 id="dag의-명확한-목적-정의">DAG의 명확한 목적 정의</h4>

<p>DAG를 만들기 전에, 당신은 당신이 그것에서 무엇을 기대하는지 생각해야합니다. 
도움이 될 수 있는 몇 가지 질문이 있습니다.</p>

<ul>
  <li>입력은 무엇입니까?</li>
  <li>무엇이 출력 될까요?</li>
  <li>언제 트리거해야합니까?</li>
  <li>어느 시간 간격으로?</li>
  <li>상호 작용할 타사 도구는 무엇입니까?</li>
  <li>작업은 무엇입니까? (매우 중요)</li>
  <li>간단하게 만들 수 있습니까?</li>
</ul>

<p>마지막 질문이 중요합니다. 
원하는 것을 달성하기 위해 DAG를 빌드해야하는지 또는 빌드 및 유지 관리가 
더 쉬운 다른 솔루션이 있는지 항상 자문 해보십시오.
데이터 파이프라인을 지나치게 복잡하게 만들지 마십시오.
DAG에는 데이터를 데이터 웨어하우스로 내보내거나 필요한 경우 프로덕션에서 
기계 학습 모델을 업데이트하는 것과 같은 명확한 목적이 있어야 합니다.</p>

<h4 id="task에-대한-명확한-목적-정의">Task에 대한 명확한 목적 정의</h4>
<p>DAG를 통해 실행하려는 작업을 명확하게 파악합니다. 
하나의 작업(task) = 여러 작업이 아닌 
하나의 작업 = 하나의 작업을 갖도록 
가능한 한 설계하십시오. 
예를 들어 보겠습니다..</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DON'T
</span><span class="k">def</span> <span class="nf">process_data</span><span class="p">():</span>
  <span class="n">wrangling</span><span class="p">()</span>
  <span class="n">cleaning</span><span class="p">()</span>
  <span class="n">transforming</span><span class="p">()</span>
  <span class="n">validating</span><span class="p">()</span>

<span class="n">task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
  <span class="n">task_id</span><span class="o">=</span><span class="s">"my_task"</span><span class="p">,</span>
  <span class="n">python_callable</span><span class="o">=</span><span class="n">process_data</span><span class="p">,</span>
  <span class="n">dag</span><span class="o">=</span><span class="n">dag</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DO
t1 = PythonOperator(
  task_id="wrangling",
  python_callable=wrangling,
)
t2 = PythonOperator(
  task_id="cleaning",
  python_callable=cleaning,
)
t3 = PythonOperator(
  task_id="transforming",
  python_callable=transforming,
)
t4 = PythonOperator(
  task_id="validating",
  python_callable=validating,
)

</code></pre></div></div>

<p>위의 예에서 볼 수 있듯이 모든 것을 
PythonOperator에 넣고 싶지만 강력히 권장하지 않습니다. 
유효성 검사 함수가  실패하는 반면 wrangling, cleaning및 transforming은 성공한다고 가정 해 봅시다  . 하나의 기능만 실패하기 때문에 Airflow는 전체 작업을 실패로 간주합니다. 따라서 유효성 검사를 수행하는  하나의 함수만 다시 시도하는 합니다. 그러나 모든 작업을 하나의 작업에 넣을 때 모든 함수가 다시 시도됩니다.</p>

<p>그렇기 때문에</p>

<ul>
  <li>one Task = one Operator</li>
</ul>

<p>어야합니다. 무언가가 실패하면 이 부분만 다시 실행되고 다른 부분은 실행되지 않습니다. 이렇게 하면 데이터의 불일치를 방지하고 실패한 항목과 그 이유를 더 쉽게 파악할 수 있습니다.</p>

<h4 id="dag-및-컨텍스트-관리자">DAG 및 컨텍스트 관리자</h4>
<p>Python에서는 컨텍스트 관리자를 활용하여 원할 때 정확하게 리소스를 할당하고 
해제할 수 있습니다. 
가장 널리 사용되는 컨텍스트 관리자는 <strong>with</strong> 입니다.
예를 들어 보겠습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DON'T
dag = DAG("simple_pipe", default_args=default_args, schedule_interval="*/5 * * * *", catchup=False) as dag:
t1 = PythonOperator(
  task_id="t1",
  python_callable=my_func
  dag=dag
)
t2 = PythonOperator(
  task_id="t2",
  python_callable=my_func
  dag=dag
)
t3 = PythonOperator(
  task_id="t3",
  python_callable=my_func
  dag=dag
)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DO
with DAG("simple_pipe", default_args=default_args, schedule_interval="*/5 * * * *", catchup=False) as dag:
  t1 = PythonOperator(
      task_id="t1",
      python_callable=my_func
  )
  t2 = PythonOperator(
      task_id="t2",
      python_callable=my_func
  )
  t3 = PythonOperator(
      task_id="t3",
      python_callable=my_func
  )
</code></pre></div></div>

<p><strong>with</strong> 문은  각 작업에 dag 변수를 할당할 필요성을 제거하여 코드를 더 깔끔하게 만듭니다. 
이렇게하면 DAG  에 속한 작업을 명확하게 볼 수 있으며 (DAG 파일에 python 함수가있는 경우 
상황이 빠르게 지저분 해질 수 있음) 컨텍스트 관리자가 DAG 수명주기를 처리하도록합니다</p>

<h4 id="default-arguments">Default arguments</h4>

<p>Task의 생성자는 이메일, 재시도 횟수, 시작 날짜, 큐 등과 같은 다양한 인수를 허용합니다. 아래와 같이 DAG 개체 정의 바로 앞에 사전이있는 많은 DAG를 이미 보았을 것입니다</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DON'T
with DAG('dag', 
  start_date=datetime(2019, 1, 1), 
  schedule_interval='*/10 * * * *', 
  catchup=False
):
  t0 = DummyOperator(task_id='t0', retries=2, retry_delay=timedelta(minutes=5))
  t1 = DummyOperator(task_id='t1', retries=2, retry_delay=timedelta(minutes=5))
  t2 = DummyOperator(task_id='t2', retries=2, retry_delay=timedelta(minutes=5))
  t3 = DummyOperator(task_id='t3', retries=2, retry_delay=timedelta(minutes=5))
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DO
default_args = {
  'retries': 1,
  'retry_delay': timedelta(minutes=5)
}

with DAG('dag', start_date=datetime(2019, 1, 1), default_args=default_args, schedule_interval='*/10 * * * *', catchup=False):
    t0 = DummyOperator(task_id='t0')
    t1 = DummyOperator(task_id='t1')
    t2 = DummyOperator(task_id='t2')
    t3 = DummyOperator(task_id='t3')

</code></pre></div></div>

<p>일반적으로 default_args 라고 하는 
이 사전의 목적은 DAG의 모든 작업에 공통적인 매개 변수 집합을 정의하는 것입니다.
이렇게 하면 동일한 인수를 반복해서 반복하지 않아도 되므로 DAG가 더 명확해지고 오류가 발생할 가능성이 줄어듭니다. 모든 작업에 대한 사전을 사용하여 기본 인수 집합을 정의하고 한 작업에 특정 값이 필요한 경우 연산자 정의에서 해당 인수를 덮어씁니다.</p>

<h4 id="의미있는-고유-식별자-또는-dag-id-정의">의미있는 고유 식별자 또는 DAG ID 정의</h4>

<p>DAG 개체를 인스턴스화할 때 DAG ID를 지정해야  합니다.  DAG ID는 모든 DAG에서 고유해야 합니다. DAG ID가 동일한 두 개의 DAG가 있으면 안 되며, 그렇지 않으면 하나의 DAG만 표시되고 예기치 않은 동작이 발생할 수 있습니다. DAG에 대한 설명과 함께 의미 있는 DAG ID를 정의합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DON'T
with DAG(
  'dag_1', 
  start_date=datetime(2019, 1 ,1), 
  schedule_interval='*/10 * * * *')
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DO
with DAG('csv_to_datawarehouse', 
  description='Fetch data from CSV, process and load them in the data warehouse' 
  start_date=datetime(2019, 1 ,1), 
  schedule_interval='*/10 * * * *')
</code></pre></div></div>

<p>수백 개의 서로 다른 DAG가 있을 때를 생각해 보십시오. 의미 있는 DAG ID와 설명을 통해 어떤 DAG가 어떤 작업을 수행하는지 빠르게 알 수 있습니다. 한 가지 더, 여러 DAG가 서로 어느 정도 관련되어있는 경우 모든 DAG에 공통 접두사를 넣는 것이 좋습니다.</p>

<h4 id="start_date">start_date</h4>
<p>여기에 큰 주제가 있습니다. 
Airflow에서 DAG를 예약하는 방식은 처음에는 이해하기 다소 어려울 수 있습니다. 
<strong>start_date</strong> DAG가 예약되기 시작하는 날짜를 정의합니다. 
한 가지 알아야 할 것은 각 작업 또는 운영자가 다른 <strong>start_date</strong> 가질 수 있다는 것입니다. 예, 동일한 DAG 내의 작업은 다른 날짜를 시작할 수 있습니다. 나는 이것에 대해 강력히 조언합니다. 
다른 시작 날짜를 정의하지 마십시오. 
일을 단순하고 규칙적으로 정의 하십시오.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DON'T
default_args = {
  'retries': 1,
  'retry_delay': timedelta(minutes=5)
}

with DAG(
  dag_id = 'dag', 
  start_date=datetime(2019, 1, 1), 
  default_args=default_args, 
  schedule_interval='*/10 * * * *', 
  catchup=False
) as dag:

  t0 = DummyOperator(task_id='t0', start_date=datetime(2019, 1, 15))
  t1 = DummyOperator(task_id='t1', start_date=datetime(2019, 2, 16))
  t2 = DummyOperator(task_id='t2', start_date=datetime(2019, 3, 6))
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DO
default_args = {
  'retries': 1,
  'retry_delay': timedelta(minutes=5),
  'start_date': datetime(2019, 1, 1)
}

with DAG(dag_id = 'dag', 
  default_args=default_args, 
  schedule_interval='*/10 * * * *', 
  catchup=False
) as dag:
  t0 = DummyOperator(task_id='t0')
  t1 = DummyOperator(task_id='t1')
  t2 = DummyOperator(task_id='t2')
  t3 = DummyOperator(task_id='t3')

  t0 &gt;&gt; [t1, t2] &gt;&gt; t3
</code></pre></div></div>

<p><strong>start_date</strong> 는 정적이어야 합니다. 
 혼란 스럽기 때문에 datetime.now() 와 같은 함수로 동적 시작 날짜를 정의하지 마십시오. 
 작업은 start_date + schedule_interval이 전달되면 실행됩니다. 
 시작 날짜가 now()로 설정되면 이론적으로 날짜가 지속적으로 앞으로 이동하므로 
 작업이 실행되지 않습니다.</p>

<h4 id="catchup-parameter">catchup parameter</h4>

<p>Airflow는 최근에 실행된 DAG 실행과 현재 날짜 사이에 트리거되지 않은 DAG 실행을 자동으로 실행합니다.  어떤 이유로 DAG를 일시 중지하고 지연을 따라 잡으려는 경우 매우 유용합니다. 그럼에도 불구하고 이 기능에 주의해야 합니다. 실제로 DAG의 start_date 1년 전으로 설정되어 있고 일정 간격이 10분으로 설정되어 있다고 가정해 보겠습니다. 해당 DAG 예약을 시작하면 수천 개의 DAG 실행이 동시에 실행됩니다. 이로 인해 다른 작업이 실행되지 않거나 Airflow 인스턴스 속도가 느려질 수 있습니다. 이를 방지하려면 기본적으로이 매개 변수를 False로 하는 것이 좋습니다. DAG 정의 또는 구성 파일에서 catchup 매개 변수를 False로 설정합니다</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>default_args = {
  'description': 'Fetch data from CSV, process and load them in the data warehouse',
  'start_date': datetime(2019, 1 ,1), 
  'schedule_interval' : '*/10 * * * *',
  'catchup': False
}

with DAG(
  dag_id = 'csv_to_datawarehouse', 
  default_args = default_args
)
</code></pre></div></div>

<p>Airflow CLI에서 에어플로우 백필  명령을 계속 사용할 수 있습니다. 
제 생각에는 이것이 트리거되지 않은 DAG 실행을 따라 잡는 가장 좋은 방법입니다.</p>

<h4 id="schedule_interval">schedule_interval</h4>

<p>일정 간격은 DAG가 트리거되는 시간 간격을 정의합니다. Airflow는 데이터 스트리밍 솔루션이 아니므로 일정 간격을 1초로 설정하지 마세요.
CRON 표현식이나 타임 델타 객체로 정의 된 schedule_interval 이미 보셨을 것입니다</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># CRON EXPRESSION
with DAG(
  'dag', 
  default_args=default_args, 
  schedule_interval='*/10 * * * *', 
  catchup=False
) as dag:
  t0 = DummyOperator(task_id='t0')

# TIMEDELTA OBJECT

with DAG(
  dag_id = 'dag', 
  default_args=default_args, 
  schedule_interval=timedelta(minutes=10),
  catchup=False
) as dag:
  t0 = DummyOperator(task_id='t0')
</code></pre></div></div>

<p>Cron 표현은 매우 강력하지만 처음에는 이해하기 어려울 수 있습니다. 다음 웹 사이트를 살펴보고  일정 간격이 예상한 간격인지 확인하십시오.</p>

<p>이제 Cron 표현식 대신 Timedelta 객체를 언제 사용해야합니까? 특정 schedule interval은  Cron 표현식으로 표현할 수 없습니다. 3일에 한 번씩 DAG를 트리거하려면 timedelta(일=3)  를 사용하여 시간 델타 개체를 정의해야 합니다. Cron 표현식을 사용하여 수행하려고하면 월말에 해당 DAG가 30 일 또는 31  일에 트리거 된 다음 다음 달 1 일에  트리거되어 3 일 간격을 깨뜨립니다.</p>

<p>즉 cron expression으로는 3일 간격으로 처리할 수 없습니다.</p>

<p>요약하자면, 이전 간격과 관련하여 일정 간격을 수행해야 한다면 timedelta 객체를 사용하십시오.</p>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[좀더 낳은 DAG 작성 DAG는 데이터 파이프라인에 해당합니다. DAG는 매일 사용되므로 모범 사례를 따르는 것이 중요합니다. 최적화되고, 이해하기 쉽고, 문서화되고, 잘 조직되어야 합니다. 수백 개의 DAG로 빠르게 끝날 수 있으므로이 부분을 과소 평가하지 마십시오. 그것은 당신에게 많은 고통과 문제를 덜어 줄 것입니다.]]></summary></entry><entry><title type="html">Create your first Airflow DAG</title><link href="http://localhost:4000/workflow/myfirst-dag/" rel="alternate" type="text/html" title="Create your first Airflow DAG" /><published>2022-12-14T00:00:00+09:00</published><updated>2022-12-14T00:00:00+09:00</updated><id>http://localhost:4000/workflow/myfirst-dag</id><content type="html" xml:base="http://localhost:4000/workflow/myfirst-dag/"><![CDATA[<h2 id="my-first-dag-개발">My First DAG 개발</h2>
<p>Apache Airflow Dag 개발 절차는 다음의 7단계 절차로 구현합니다.</p>

<ul>
  <li>Airflow 관련 Module import</li>
  <li>DAG Arguments 정의</li>
  <li>Python Function 또는 task 에서 사용하는 Variable 정의 (Optional)</li>
  <li>Instatiate DAG 정의</li>
  <li>Task 정의</li>
  <li>Task간 의존성 정의</li>
  <li>Verify DAG</li>
</ul>

<p>개발절차를 예제로 살펴보면 다음과 같습니다.</p>

<h4 id="1-airflow-관련-module-import">1. Airflow 관련 Module import</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
</code></pre></div></div>

<h4 id="2-dag-arguments-정의">2. DAG Arguments 정의</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="3-python-function-또는-task-에서-사용하는-variable-정의optional">3. Python Function 또는 task 에서 사용하는 Variable 정의(Optional)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hello_airflow</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Hello airflow"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="4-instantiate-dag-정의">4. Instantiate DAG 정의</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span> <span class="o">=</span> <span class="s">"myFirstDag"</span><span class="p">,</span>
  <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">,</span>
  <span class="n">dags</span> <span class="o">=</span> <span class="p">[</span><span class="s">'training'</span><span class="p">]</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
</code></pre></div></div>

<h4 id="5--task-정의">5.  Task 정의</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">t1</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">"bash"</span><span class="p">,</span>
    <span class="n">bash_command</span><span class="o">=</span><span class="s">"echo Hello airflow"</span><span class="p">,</span>
  <span class="p">)</span>
  
  <span class="n">t2</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">"python"</span><span class="p">,</span>
    <span class="n">python_callable</span><span class="o">=</span><span class="n">hello_airflow</span><span class="p">,</span>
  <span class="p">)</span>
</code></pre></div></div>

<h4 id="6-task간-의존성-정의">6. Task간 의존성 정의</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  t1 &gt;&gt; t2
</code></pre></div></div>

<h4 id="7-verify-dag">7. Verify DAG</h4>

<p>myFirstDag을 airflow에 배포하여 dag을 테스트합니다. 
테스트 aifrlow Webserver UI에서 하거나 airflow cli를 사용하여 테스트합니다.</p>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[My First DAG 개발 Apache Airflow Dag 개발 절차는 다음의 7단계 절차로 구현합니다.]]></summary></entry><entry><title type="html">Airflow Task</title><link href="http://localhost:4000/workflow/tasks/" rel="alternate" type="text/html" title="Airflow Task" /><published>2022-11-27T00:00:00+09:00</published><updated>2022-11-27T00:00:00+09:00</updated><id>http://localhost:4000/workflow/tasks</id><content type="html" xml:base="http://localhost:4000/workflow/tasks/"><![CDATA[<h2 id="airflow-task">Airflow Task</h2>
<p>Task는 airflow의 기본 실행단위로 한개 이상의 Task를 이용해서 하나의 DAG을 정의합니다. Task간 순서를 표현하기 위해 작업간 «(스트림업), »(스트림다운) 종속성을 설정하여 합니다.
Task는</p>

<ul>
  <li>Operator : 지정한 작읍을 수행하는 Operator</li>
  <li>Sensor : 어떤 조건이 만족하는지 주기적으로 스캔이 필요할 때 사용하며 조건이 만족하는 경우 Task가 수행.</li>
  <li>Hook : DB나 서비스 같은 외부 시스템과 통신하기 위한 인터페이스를 제공하여 연결 상태를 유지
등을 사용할 수 있습니다.</li>
</ul>

<h2 id="task-instance">Task Instance</h2>

<p>DAG실행될 때 마다 Task Instance를 생성하여 Executor로 전달하여 해당작업을 실행합니다. 그리고 해당 Task Instance를 다시 Metadata로 보내서 상태를 업데이트하며, Task Instance의 작업이 남아 있으면 Executor로 다시 보내집니다. 작업이 완료가 되면 스케줄러에게 보냅니다.
Operator
Operator는 task를 어떻게 실행시킬지 정의합니다. 하나의 워크플로우안에서 한개 이상의 task를 정의할 수 있습니다. 하나의 Task가 하나의 Operator라고 할 수 있다.
Operator는 Action Operator와 Transfer Operator로 구분됩니다.</p>

<ul>
  <li>Action Operator : 작업을 수행하거나 다른 시스템에 작업을 수행하도록 trigger합니다.</li>
  <li>Transfer Operator : 특정 시스템에 다른 시스템으로 데이터를 이동</li>
  <li>Sensor Operator : 특정 조건에 일치할 때 까지 기다렸다가, 만족되면 이후 과정을 진행하도록 기다려는 Operator.</li>
</ul>

<p>Airflow는 기본 Operator는 Bash와 Python Operator가 대표적이며 그외 많은 Operator를 지원하고 있습니다. Operator에 공통적으로 **kwargs라는 keywoard Arguments를 전달하는 부분이 있으며, DAG을 정의할 때 default_args 전달하는 것처럼 전달합니다.</p>

<h2 id="task-dependencies">Task Dependencies</h2>
<p>Apache Airflow의 DAG 내에 task들의 dependency를 설정함으로써 task 실행 순서와 병렬 실행 task들 등을 정의할 수 있습니다.
Task 간 의존성은 다음과 같이</p>
<ul>
  <li>set_downstream 또는 » 기호</li>
  <li>set_upstream 또는 « 기호 
같은 함수 또는 기호로 설정할 수 있습니다. 
set_downstream 는 Task 실행 후에 수행할 task를 설정
set_upstream 는 Task 실행 전에 수행할 task를 설정</li>
</ul>

<p>예시)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>

<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span> 
<span class="kn">from</span> <span class="nn">textwrap</span> <span class="kn">import</span> <span class="n">dedent</span> 

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span>

<span class="n">dag</span> <span class="o">=</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="s">'data_pipeline_ex09'</span><span class="p">,</span>
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span><span class="p">,</span>
  <span class="n">description</span><span class="o">=</span><span class="s">'Hello world'</span><span class="p">,</span>
  <span class="n">schedule_interval</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">hello</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Hello!'</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span>  <span class="n">BashOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">'echo_hello'</span><span class="p">,</span>
    <span class="n">bash_command</span><span class="o">=</span><span class="s">'echo "Hi from bash operator"'</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span>
<span class="p">)</span>

<span class="n">t2</span> <span class="o">=</span> <span class="n">python_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">"python_task"</span><span class="p">,</span>
    <span class="n">python_callable</span><span class="o">=</span><span class="n">hello</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span>
<span class="p">)</span>

<span class="n">templated_command</span> <span class="o">=</span> <span class="n">dedent</span><span class="p">(</span>
  <span class="s">"""
  
  """</span>
<span class="p">)</span>

<span class="n">t3</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
  <span class="n">task_id</span><span class="o">=</span><span class="s">'templated'</span><span class="p">,</span>
  <span class="n">bash_command</span><span class="o">=</span><span class="n">templated_command</span><span class="p">,</span>
  <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s">'my_param'</span><span class="p">:</span> <span class="s">'Parameter I passed in'</span><span class="p">},</span>
  <span class="n">dag</span><span class="o">=</span><span class="n">dag</span>
<span class="p">)</span>

<span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="n">t2</span> <span class="o">&gt;&gt;</span> <span class="n">t3</span>
<span class="c1"># t1.set_downstream(t2) 
# t3.set_upstream(t2)
</span></code></pre></div></div>

<p>t1, t2, t3 task가 순차적으로 실행됩니다.</p>

<p>t1.set_downstream(t2)</p>

<p>t2는 성공적으로 실행되는 t1에 의존하여 실행됩니다.</p>

<p>t2.set_upstream(t1)</p>

<p>비트 시프트 연산자를 사용하여 작업을 연결할 수도 있습니다.:
t1 » t2</p>

<p>그리고 비트 시프트 연산자와의 업스트림 종속성 표기:
t2 « t1</p>

<p>여러 종속성을 연결하는 것은 비트 시프트 연산자로 간결해집니다.
t1 » t2 » t3</p>

<p>작업 목록을 종속성으로 설정할 수도 있습니다. 
이러한 작업은 모두 동일한 효과를 갖습니다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span><span class="p">.</span><span class="n">set_downstream</span><span class="p">([</span><span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">])</span>
<span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="p">[</span><span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">]</span>
<span class="p">[</span><span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="n">t1</span>
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><category term="task" /><summary type="html"><![CDATA[Airflow Task Task는 airflow의 기본 실행단위로 한개 이상의 Task를 이용해서 하나의 DAG을 정의합니다. Task간 순서를 표현하기 위해 작업간 «(스트림업), »(스트림다운) 종속성을 설정하여 합니다. Task는]]></summary></entry><entry><title type="html">Airflow XCom</title><link href="http://localhost:4000/workflow/xcom/" rel="alternate" type="text/html" title="Airflow XCom" /><published>2022-11-27T00:00:00+09:00</published><updated>2022-11-27T00:00:00+09:00</updated><id>http://localhost:4000/workflow/xcom</id><content type="html" xml:base="http://localhost:4000/workflow/xcom/"><![CDATA[<h2 id="airflow-xcom">Airflow XCom</h2>
<p>Airflow 작업(Task) 간에 데이터를 전달하는 첫 번째 방법은 작업 데이터를 공유하기 위한 주요 Airflow 기능인 XCom을 사용하는 것입니다.
XCom은 task간 데이터를 공유가 필요할 때,  데이터를 공유하기 위해 push, pull 을 사용하여 값을 전달하고, 값을 가져올 수 있습니다.. 
XComs는 작업에서 보내는 의미의 “푸시” , 작업에서 수신하는 것을 의미하는 “pulled”일 수 있습니다. 푸시된 XCom은 에어플로우 메타데이터 데이터베이스에 저장되고 다른 모든 작업에서 사용할 수 있게 됩니다. 작업이 값을 반환할 때마다.
Airflow에서는 여러 분산환경에서 서로 다른 Worker에서 Task가 실행 될 수 있기 때문에 Xcom을 사용합니다. Variable과 비슷하지만 Xcom은 특정 DAG내부에서만 공유되는 특징이 있습니다. 여러 DAG에서 공유해서 사용하려면 Variable을 사용해야 합니다.  PythonOperator를 사용하면 return값이 자동으로 Xcom에 push됩니다.</p>

<p>Airflow UI의 Admin &gt; XComs 메뉴에서XCom을 내용을 볼 수 있습니다. 다음과 같은 내용이 표시되어야 합니다.</p>

<figure style="width: 100%" class="align-left">
  <img src="http://localhost:4000/assets/images/07-xcom_ui.png" alt="" />
  <figcaption></figcaption>
</figure>

<h4 id="xcom-사용시기-및-제약사항">XCom 사용시기 및 제약사항</h4>
<p>XComs는 작업 간에 소량의 데이터를 전달하는 데 사용해야 합니다. 예를 들어 작업 메타데이터, 날짜, 모델 정확도 또는 단일 값 쿼리 결과는 모두 XCom과 함께 사용하기에 이상적인 데이터입니다.
XCom으로 작은 데이터 세트를 전달하는 것을 막을 수는 없지만 그렇게 할 때는 매우 주의하십시오. 이것은 XCom이 설계된 목적이 아니며 팬더 데이터 프레임과 같은 데이터를 전달하는 데 사용하면 DAG의 성능이 저하되고 메타데이터 데이터베이스의 저장소를 차지할 수 있습니다.
XCom은 작업 간에 큰 데이터 세트를 전달하는 데 사용할 수 없습니다. XCom의 크기 제한은 사용 중인 메타데이터 데이터베이스에 따라 결정됩니다</p>

<ul>
  <li>Postgres: 1 Gb</li>
  <li>SQLite: 2 Gb</li>
  <li>MySQL: 64 Kb</li>
</ul>

<p>이러한 한계가 그리 크지 않다는 것을 알 수 있습니다. 데이터가 최대 허용 한도를 충족한다고 생각되더라도 XComs를 사용하지 마십시오. 대신 더 많은 양의 데이터에 더 적합한 중간 데이터 저장소를 사용하십시오.</p>

<h4 id="custom-xcom-backends">Custom XCom Backends</h4>

<p>커스텀 XCom 백엔드는 에어플로우 2.0 이상에서 사용할 수 있는 새로운 기능입니다. XCom 백엔드를 사용하면 Airflow의 메타데이터 데이터베이스의 기본값이 아닌 S3, GCS 또는 HDFS와 같은 외부 시스템에서 XCom을 푸시하고 풀 할 수 있습니다. 또한 사용자 고유의 직렬화 및 역직렬화 메서드를 구현하여 XCom이 처리되는 방법을 정의할 수 있습니다. 이것은 그 자체로 개념이며 사용자 지정 XCom 백엔드를 읽으면 더 많은 것을 배울 수 있습니다.</p>

<p>예시)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">push_func</span><span class="p">(</span><span class="o">**</span><span class="n">context</span><span class="p">):</span>
  <span class="n">contenxt</span><span class="p">[</span><span class="err">‘</span><span class="n">task_instance</span><span class="err">’</span><span class="p">].</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">변수명</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">전달할</span> <span class="n">value</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pull_func</span><span class="p">(</span><span class="o">**</span><span class="n">context</span><span class="p">):</span>
  <span class="n">value</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="err">‘</span><span class="n">ti</span><span class="err">’</span><span class="p">].</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">변수명</span><span class="p">,</span> <span class="n">task_ids</span><span class="o">=</span><span class="n">대상</span> <span class="n">Task이름</span><span class="p">)</span>

</code></pre></div></div>
<ul>
  <li>Xcom 예시 1)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span> 
<span class="kn">from</span> <span class="nn">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>

<span class="c1"># Utils 
</span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span><span class="n">timedelta</span> 
<span class="kn">from</span> <span class="nn">airflow.utils.dates</span> <span class="kn">import</span> <span class="n">days_ago</span> 

<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">'start_date'</span><span class="p">:</span> <span class="n">days_ago</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
  <span class="s">'schedule_interval'</span><span class="p">:</span> <span class="s">'@daily'</span><span class="p">,</span>
  <span class="s">'tags'</span><span class="p">:</span> <span class="s">'training'</span><span class="p">,</span>
  <span class="s">'catchup'</span><span class="p">:</span> <span class="bp">False</span>
<span class="p">}</span> 

<span class="k">def</span> <span class="nf">xcom_push</span><span class="p">(</span><span class="o">**</span><span class="n">context</span><span class="p">):</span>
  <span class="n">context</span><span class="p">[</span><span class="s">'task_instance'</span><span class="p">].</span><span class="n">xcom_push</span><span class="p">(</span>
    <span class="n">key</span><span class="o">=</span><span class="s">'pushed_value'</span><span class="p">,</span>
    <span class="n">value</span><span class="o">=</span><span class="s">'xcom_push_test_message!'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pull_func</span><span class="p">(</span><span class="o">**</span><span class="n">context</span><span class="p">):</span>
  <span class="n">value</span><span class="o">=</span><span class="n">context</span><span class="p">[</span><span class="s">'ti'</span><span class="p">].</span><span class="n">xcom_pull</span><span class="p">(</span>
    <span class="n">key</span><span class="o">=</span><span class="s">'pushed_value'</span><span class="p">,</span> 
    <span class="n">task_ids</span><span class="o">=</span><span class="s">'push_by_xcom'</span>
  <span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
  <span class="n">dag_id</span><span class="o">=</span><span class="s">'xcom_dag'</span><span class="p">,</span> 
  <span class="n">default_args</span> <span class="o">=</span> <span class="n">default_args</span> 
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

  <span class="n">push_by_xcom</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">'push_by_xcom'</span><span class="p">,</span>
    <span class="n">python_callable</span><span class="o">=</span><span class="n">xcom_push</span>
  <span class="p">)</span>
  
  <span class="n">pull_task1</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">'pull_example1'</span><span class="p">,</span>
    <span class="n">python_callable</span><span class="o">=</span><span class="n">pull_func</span>
  <span class="p">)</span>
  
  <span class="n">pull_task2</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s">'pull_example2'</span><span class="p">,</span>
    <span class="n">bash_command</span><span class="o">=</span><span class="s">'echo ""'</span>
  <span class="p">)</span>
  
  <span class="n">push_by_xcom</span> <span class="o">&gt;&gt;</span> <span class="n">pull_task1</span> <span class="o">&gt;&gt;</span> <span class="n">pull_task2</span>
</code></pre></div></div>

<ul>
  <li>Xcom 예시 2)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python_operator</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">url</span> <span class="o">=</span> <span class="s">'https://covidtracking.com/api/v1/states/'</span>
<span class="n">state</span> <span class="o">=</span> <span class="s">'wa'</span>

<span class="k">def</span> <span class="nf">get_testing_increase</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">ti</span><span class="p">):</span>
    <span class="s">"""
    Gets totalTestResultsIncrease field from Covid API for given state and returns value
    """</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="o">+</span><span class="s">'{0}/current.json'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">testing_increase</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">text</span><span class="p">)[</span><span class="s">'totalTestResultsIncrease'</span><span class="p">]</span>

    <span class="n">ti</span><span class="p">.</span><span class="n">xcom_push</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s">'testing_increase'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">testing_increase</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">analyze_testing_increases</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">ti</span><span class="p">):</span>
    <span class="s">"""
    Evaluates testing increase results
    """</span>
    <span class="n">testing_increases</span><span class="o">=</span><span class="n">ti</span><span class="p">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s">'testing_increase'</span><span class="p">,</span> <span class="n">task_ids</span><span class="o">=</span><span class="s">'get_testing_increase_data_{0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Testing increases for {0}:'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="n">testing_increases</span><span class="p">)</span>
    <span class="c1">#run some analysis here
</span>
<span class="c1"># Default settings applied to all tasks
</span><span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'owner'</span><span class="p">:</span> <span class="s">'airflow'</span><span class="p">,</span>
    <span class="s">'depends_on_past'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
    <span class="s">'email_on_failure'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
    <span class="s">'email_on_retry'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
    <span class="s">'retries'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">'retry_delay'</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span><span class="s">'xcom_dag'</span><span class="p">,</span>
         <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2021</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
         <span class="n">max_active_runs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
         <span class="n">schedule_interval</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span>
         <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">,</span>
         <span class="n">catchup</span><span class="o">=</span><span class="bp">False</span>
         <span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>

    <span class="n">opr_get_covid_data</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span> <span class="o">=</span> <span class="s">'get_testing_increase_data_{0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">state</span><span class="p">),</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">get_testing_increase</span><span class="p">,</span>
        <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span><span class="n">state</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="n">opr_analyze_testing_data</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span> <span class="o">=</span> <span class="s">'analyze_data'</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">analyze_testing_increases</span><span class="p">,</span>
                <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span><span class="n">state</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="n">opr_get_covid_data</span> <span class="o">&gt;&gt;</span> <span class="n">opr_analyze_testing_data</span>
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="workflow" /><category term="airflow" /><summary type="html"><![CDATA[Airflow XCom Airflow 작업(Task) 간에 데이터를 전달하는 첫 번째 방법은 작업 데이터를 공유하기 위한 주요 Airflow 기능인 XCom을 사용하는 것입니다. XCom은 task간 데이터를 공유가 필요할 때, 데이터를 공유하기 위해 push, pull 을 사용하여 값을 전달하고, 값을 가져올 수 있습니다.. XComs는 작업에서 보내는 의미의 “푸시” , 작업에서 수신하는 것을 의미하는 “pulled”일 수 있습니다. 푸시된 XCom은 에어플로우 메타데이터 데이터베이스에 저장되고 다른 모든 작업에서 사용할 수 있게 됩니다. 작업이 값을 반환할 때마다. Airflow에서는 여러 분산환경에서 서로 다른 Worker에서 Task가 실행 될 수 있기 때문에 Xcom을 사용합니다. Variable과 비슷하지만 Xcom은 특정 DAG내부에서만 공유되는 특징이 있습니다. 여러 DAG에서 공유해서 사용하려면 Variable을 사용해야 합니다. PythonOperator를 사용하면 return값이 자동으로 Xcom에 push됩니다.]]></summary></entry><entry><title type="html">Cloud-Native CI/CD 이해</title><link href="http://localhost:4000/devops/cloud-native-cicd/" rel="alternate" type="text/html" title="Cloud-Native CI/CD 이해" /><published>2022-11-27T00:00:00+09:00</published><updated>2022-11-27T00:00:00+09:00</updated><id>http://localhost:4000/devops/cloud-native-cicd</id><content type="html" xml:base="http://localhost:4000/devops/cloud-native-cicd/"><![CDATA[<h2 id="cloud-native-cicd-이해">Cloud-Native CI/CD 이해</h2>
<p>클라우드 네이티브 소프트웨어 개발이 무엇을 의미하는지 더 잘 이해 했으므로 CI/CD 파이프 라인의 맥락에서 그것이 무엇을 의미하는지 살펴 보겠습니다.</p>

<p>Cloud-Native CI/CD는 세 가지 원칙을 기반</p>

<ul>
  <li><strong>Containers</strong></li>
  <li><strong>Serverless</strong></li>
  <li><strong>DevOps</strong></li>
</ul>

<p><strong>Containers</strong></p>

<p>CI/CD의 컨텍스트에서 클라우드 네이티브는 모든 것이 컨테이너 내에서 실행되어야 함을 의미합니다. 응용 프로그램을 테스트하거나 패키지하기 위해 코드베이스에서 완료되는 각 작업은 자체 격리된 컨테이너에서 수행해야 합니다.
이러한 컨테이너를 사용하면 모든 팀 멤버 또는 자동화된 시스템이 동일한 작업을 실행하고 동일한 예측 가능한 최종 결과를 얻을 수 있습니다.
이는 또한 일부 소스 코드에서 특정 작업을 실행하는 데 필요한 모든 런타임 및 구성이 파이프라인이 실행될 때마다 항상 동일하다는 것을 의미합니다. 이렇게 하면 파이프라인의 안정성이 향상되며 필요한 도구를 설치하는 데 시스템 관리자의 도움이 필요하지 않습니다.</p>

<p><strong>Serverless</strong></p>

<p>클라우드 네이티브 CI/CD에 대해 이야기할 때 서버리스는 Azure 함수 또는 AWS 람다와 같은 서비스로서의 함수를 의미하지 않습니다. 중앙 CI 엔진을 유지 관리하고 관리할 필요 없이 온디맨드로 실행하고 확장하는 것입니다.
소프트웨어 개발자는 리소스 할당 내에서 파이프라인을 효율적이고 신속하게 편집하고 실행할 수 있어야 합니다. 파이프라인을 관리하는 중앙 시스템에서 관리자 권한이 필요하지 않습니다. 클라우드 네이티브 CI/CD 솔루션이 성공하려면 모든 시스템 사용자가 액세스하고 관리할 수 있어야 합니다.</p>

<p>Tekton은 CI/CD(지속적 통합 및 지속적 전달) 시스템을 만들기 위한 Kubernetes 네이티브 오픈 소스 프레임워크입니다. 여러 클라우드 공급자 또는 하이브리드 환경에서 애플리케이션을 구축, 테스트 및 배포하는 데 최적화되어 있습니다.
Tekton은 CI/CD 파이프라인을 구축하기 위한 클라우드 네이티브 솔루션입니다. 빌딩 블록을 제공하는 Tekton 파이프 라인과 Tekton Cli 및 Tekton 카탈로그와 같은 지원 구성 요소로 구성되어 Tekton을 완벽한 생태계로 만듭니다. Tekton은 Linux Foundation 프로젝트인 CD Foundation의 일부입니다.</p>

<p><strong>DevOps</strong></p>

<p>클라우드 네이티브 CI/CD는 DevOps를 염두에 두고 구축해야 합니다. 팀이 다른 팀을 대신하여 배달 파이프라인을 관리하는 중앙 우수 센터 팀에 의존하지 않고 애플리케이션과 함께 배달 파이프라인을 소유할 수 있도록 해야 합니다.
소프트웨어 개발 팀이 파이프라인을 담당하도록 하면 파이프라인을 관리하고 항상 필요한 최신 소프트웨어를 사용하여 작업을 수행할 수 있습니다.
이것이 바로 Kubernetes에서 기본적으로 실행되는 클라우드 네이티브 CI/CD 솔루션인 Tekton을 만들게 된 원칙입니다. 다음 섹션에서는 Tekton에 대해 자세히 알아보고 클라우드 네이티브 사고 방식으로 CI/CD에 접근한 방법을 알아봅니다.</p>]]></content><author><name>Jaeguk Yun</name></author><category term="devops" /><category term="tekton" /><summary type="html"><![CDATA[Cloud-Native CI/CD 이해 클라우드 네이티브 소프트웨어 개발이 무엇을 의미하는지 더 잘 이해 했으므로 CI/CD 파이프 라인의 맥락에서 그것이 무엇을 의미하는지 살펴 보겠습니다.]]></summary></entry><entry><title type="html">Tekton First Pipeline</title><link href="http://localhost:4000/devops/tekton-first-pipeline/" rel="alternate" type="text/html" title="Tekton First Pipeline" /><published>2022-11-27T00:00:00+09:00</published><updated>2022-11-27T00:00:00+09:00</updated><id>http://localhost:4000/devops/tekton-first-pipeline</id><content type="html" xml:base="http://localhost:4000/devops/tekton-first-pipeline/"><![CDATA[<h2 id="first-pipeline-작성">First Pipeline 작성</h2>
<p>Tekton을 이용하여 Pipeline을 작성하는 것을 실습합니다.</p>

<p>Pipeline은 CI/CD 워크플로의 일부로 특정 실행 순서로 정렬된 일련의 Task를 정의합니다.</p>

<p>이번에는 first Pipeline을 작성할 것입니다, First Pipeline에서는 이전에 작성했던 Hello World! 그리고 goodbye World! Task를 포함하는 Pipeline을 작성합니다.</p>

<p>goodbye task를 다음과 같이 작성하고 적용합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> goodbye.yaml
</code></pre></div></div>

<p>[goodbye.yaml]</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">tekton.dev/v1beta1</span> 
<span class="na">kind</span><span class="pi">:</span> <span class="s">Task</span> 
<span class="na">metadata</span><span class="pi">:</span> 
  <span class="na">name</span><span class="pi">:</span> <span class="s">goodbye</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">steps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">goodbye</span> 
      <span class="na">image</span><span class="pi">:</span> <span class="s">alpine</span> 
      <span class="na">script</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">#!/bin/sh</span>
        <span class="s">echo "Goodbye World!" </span>
</code></pre></div></div>

<h2 id="pipeline-작성">Pipeline 작성</h2>
<p>hello-world 타스크와 goodbye 타스크를 연결하는 pipeline을 작성합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> hello-goodbye-pipeline.yaml
</code></pre></div></div>

<p>[hello-goodbye-pipeline.yaml]</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">tekton.dev/v1beta1</span> 
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pipeline</span> 
<span class="na">metadata</span><span class="pi">:</span> 
  <span class="na">name</span><span class="pi">:</span> <span class="s">hello-goodbye-pipeline</span> 
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">tasks</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">first-task</span> 
      <span class="na">taskRef</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">hello-world</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">goodbye-task</span>
      <span class="na">taskRef</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">goodbye</span>
</code></pre></div></div>

<h2 id="pipeline-run-작성">Pipeline Run 작성</h2>
<p>Pipeline을 인스턴스화하는 Pipeline Run을 다음과 같이 작성해서 Kubernetes에 적용합니다.</p>

<p>[hello-goodbye-pipeline-run.yaml]</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> hello-goodbye-pipeline-run.yaml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">tekton.dev/v1beta1</span> 
<span class="na">kind</span><span class="pi">:</span> <span class="s">PipelineRun</span> 
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">hello-goodbye-pipeline-run</span> 
<span class="na">spec</span><span class="pi">:</span> 
  <span class="na">pipelineRef</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">hello-goodbye-pipeline</span>
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="devops" /><category term="tekton" /><summary type="html"><![CDATA[First Pipeline 작성 Tekton을 이용하여 Pipeline을 작성하는 것을 실습합니다.]]></summary></entry></feed>