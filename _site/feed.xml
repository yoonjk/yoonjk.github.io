<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-06T19:59:34+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Cloud Native Journey</title><subtitle>Software Engineer/Architect</subtitle><author><name>Jaeguk Yun</name></author><entry><title type="html"></title><link href="http://localhost:4000/2023-06-30-install-argo-workflow/" rel="alternate" type="text/html" title="" /><published>2023-11-06T19:59:34+09:00</published><updated>2023-11-06T19:59:34+09:00</updated><id>http://localhost:4000/2023-06-30-install-argo-workflow</id><content type="html" xml:base="http://localhost:4000/2023-06-30-install-argo-workflow/"><![CDATA[<h2 id="argo-workflow-설치">Argo Workflow 설치</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-n</span> argo <span class="nt">-f</span> https://github.com/argoproj/argo-workflows/releases/download/v3.4.6/namespace-install.yaml
</code></pre></div></div>

<h2 id="argo-workflow-server-login-변경없이-접근">Argo Workflow server login 변경없이 접근</h2>
<p>Argo Workflow 3.x 이후 부터 UI에 접근하기 위해 로그인하도록 변경되었습니다.
그존 방식처럼 로그인 없이 사용하기 위해 다음과 같이 설정합니다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl patch deployment   argo-server   <span class="nt">--namespace</span> argo   <span class="nt">--type</span><span class="o">=</span><span class="s1">'json'</span>   <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": [
  "server",
  "--auth-mode=server"
]}]'</span>
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2023-08-19-Interoperability-iceberg-presto-spark/" rel="alternate" type="text/html" title="" /><published>2023-11-06T19:59:34+09:00</published><updated>2023-11-06T19:59:34+09:00</updated><id>http://localhost:4000/2023-08-19-Interoperability-iceberg-presto-spark</id><content type="html" xml:base="http://localhost:4000/2023-08-19-Interoperability-iceberg-presto-spark/"><![CDATA[<p>Setup prestodb/presto server + iceberg catalog backed by a HMS service.
Steps to Run Presto(0.282+) + Hive(3.0) + Iceberg(1.2.1 &amp; 1.3.0) and interoperability with Apache Spark 3.2.x</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://downloads.apache.org/hive/hive-standalone-metastore-3.0.0/hive-standalone-metastore-3.0.0-bin.tar.gz
<span class="nb">tar </span>xvzf hive-standalone-metastore-3.0.0-bin.tar.gz
<span class="nb">mv </span>hive-standalone-metastore-3.0.0-bin hive
<span class="nb">cd </span>hive
bin/schematool <span class="nt">-dbType</span> derby <span class="nt">-initSchema</span> <span class="c"># command will create and init the derby</span>
<span class="c"># Start the hive metadata service</span>
bin/start-metastore
</code></pre></div></div>
<p>Similarly extract the file download in step 2 and point the extracted hive location as:export HIVE_HOME=/home/user/hive</p>

<h2 id="presto-설치">Presto 설치</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.282/presto-server-0.282.tar.gz
<span class="nb">tar </span>xvzf presto-server-0.282.tar.gz
<span class="nb">mv </span>presto-server-0.282 presto
<span class="nb">export </span><span class="nv">PRESTO_HOME</span><span class="o">=</span>~/presto

<span class="c"># create the directories under presto installation dir.</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$PRESTO_HOME</span>/etc/catalog

<span class="c"># /etc/node.properties</span>
node.environment<span class="o">=</span>production
node.id<span class="o">=</span>ffffffff-ffff-ffff-ffff-ffffffffffff
node.data-dir<span class="o">=</span>/home/user/presto/data
<span class="c"># Please note replace /home/user with a path that this process can write to.</span>

<span class="c"># $PRESTO_HOME/etc/config.properties</span>
<span class="nv">coordinator</span><span class="o">=</span><span class="nb">true
</span>node-scheduler.include-coordinator<span class="o">=</span><span class="nb">true
</span>http-server.http.port<span class="o">=</span>8080
query.max-memory<span class="o">=</span>5GB
query.max-memory-per-node<span class="o">=</span>1GB
discovery-server.enabled<span class="o">=</span><span class="nb">true
</span>discovery.uri<span class="o">=</span>http://localhost:8080

<span class="c"># $PRESTO_HOME/etc/jvm.config</span>
<span class="nt">-Djdk</span>.attach.allowAttachSelf<span class="o">=</span><span class="nb">true</span>
<span class="nt">-server</span>
<span class="nt">-Xmx16G</span>
<span class="nt">-XX</span>:+UseG1GC
<span class="nt">-XX</span>:G1HeapRegionSize<span class="o">=</span>32M
<span class="nt">-XX</span>:+UseGCOverheadLimit
<span class="nt">-XX</span>:+ExplicitGCInvokesConcurrent
<span class="nt">-XX</span>:+HeapDumpOnOutOfMemoryError
<span class="nt">-XX</span>:+ExitOnOutOfMemoryError

<span class="c"># $PRESTO_HOME/etc/catalog/iceberg.properties</span>
connector.name<span class="o">=</span>iceberg
hive.metastore.uri<span class="o">=</span>thrift://localhost:9083
iceberg.catalog.type<span class="o">=</span>hive
</code></pre></div></div>
<h2 id="참고">참고</h2>
<p><a href="https://medium.com/@scrapcodes/interoperability-presto-iceberg-spark-4e5299ec7de5">Interoperability Presto + Iceberg + Spark</a></p>]]></content><author><name>Jaeguk Yun</name></author></entry><entry><title type="html"></title><link href="http://localhost:4000/2023-04-16-install-lua/" rel="alternate" type="text/html" title="" /><published>2023-11-06T19:59:34+09:00</published><updated>2023-11-06T19:59:34+09:00</updated><id>http://localhost:4000/2023-04-16-install-lua</id><content type="html" xml:base="http://localhost:4000/2023-04-16-install-lua/"><![CDATA[<h2 id="lua-설치---사전준비">lua 설치 - 사전준비</h2>
<p>사전에 compiler가 설치되어 있어야 합니다.
Compiler가 설치되어 있지 않는 경우 다음을 실행합니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo yum update -y
yum groupinstall -y 'Development Tools'
</code></pre></div></div>
<h2 id="lua-설치">lua 설치</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-R</span> <span class="nt">-O</span> http://www.lua.org/ftp/lua-5.4.4.tar.gz
<span class="nb">tar </span>zxf lua-5.4.4.tar.gz
<span class="nb">cd </span>lua-5.4.4
make all <span class="nb">test</span>
</code></pre></div></div>

<h2 id="lua-cli-실행">lua cli 실행</h2>
<p>lua를 실행하고 Hello World를 출력합니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>src/lua
str = "Hello World"
print(str)
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author></entry><entry><title type="html">Quickstart Iceberg with Spark and Docker Compose</title><link href="http://localhost:4000/watson/Quickstart-Iceberg-with-Spark-and-Docker-Compose/" rel="alternate" type="text/html" title="Quickstart Iceberg with Spark and Docker Compose" /><published>2023-08-19T00:00:00+09:00</published><updated>2023-08-19T00:00:00+09:00</updated><id>http://localhost:4000/watson/Quickstart%20Iceberg%20with%20Spark%20and%20Docker%20Compose</id><content type="html" xml:base="http://localhost:4000/watson/Quickstart-Iceberg-with-Spark-and-Docker-Compose/"><![CDATA[<p>Apache Iceberg는 대규모(페타바이트) 분석 데이터 세트를 위한 오픈 테이블 형식(데이터 파일을 구성하는 방법)입니다. 넷플릭스에서 만들었으며 ASF에서 오픈 소스화되었습니다. Netflix, Apple 및 기타 여러 회사에서 광범위하게 사용되고 있습니다. Tabular.io는 Apache Iceberg 테이블을 기반으로 합니다. Dremio.com Arctic은 아파치 아이스버그용으로 구축되었습니다.</p>

<p>두 가지 구성 요소로 구성된 오픈 테이블 형식입니다:</p>

<ul>
  <li>메타데이터 파일(메타데이터 파일, 매니페스트 목록 파일, 매니페스트 파일)
데이터 파일(데이터 자체)</li>
  <li>가장 널리 사용되는 쿼리 엔진 또는 프레임워크는 Apache Spark이며 다른 쿼리 엔진으로는 Snowflake, Trino, Starburst, 
… 등이 있습니다.</li>
</ul>

<h2 id="다음-단계-iceberg-스파크-시작">다음 단계: Iceberg, 스파크 시작</h2>
<p>Apache Iceberg를 사용해 볼 수 있는 방법은 여러 가지가 있습니다. 클라우드를 선호하는 경우, 호스팅된 Tabular.io를 사용해 볼 수 있습니다.</p>

<p>저는 docker-compose.yml을 사용하겠습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/tabular-io/docker-spark-iceberg
<span class="nb">cd </span>docker-spark-iceberg
docker-compose up <span class="nt">-d</span> 
</code></pre></div></div>

<p>새 터미널 창을 열고 spark-sql 프롬프트로 이동합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>% docker <span class="nb">exec</span> <span class="nt">-it</span> spark-iceberg spark-sql
...
Spark master: <span class="nb">local</span><span class="o">[</span><span class="k">*</span><span class="o">]</span>, Application Id: local-1682287035887
spark-sql&gt;
</code></pre></div></div>
<p>테이블 생성</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-sql&gt; CREATE TABLE demo.nyc.taxis
         <span class="o">&gt;</span> <span class="o">(</span>
         <span class="o">&gt;</span>   vendor_id bigint,
         <span class="o">&gt;</span>   trip_id bigint,
         <span class="o">&gt;</span>   trip_distance float,
         <span class="o">&gt;</span>   fare_amount double,
         <span class="o">&gt;</span>   store_and_fwd_flag string
         <span class="o">&gt;</span> <span class="o">)</span>
         <span class="o">&gt;</span> PARTITIONED BY <span class="o">(</span>vendor_id<span class="o">)</span><span class="p">;</span>
</code></pre></div></div>
<p>데이터 등록</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>park-sql&gt; INSERT INTO demo.nyc.taxis
         <span class="o">&gt;</span> VALUES <span class="o">(</span>1, 1000371, 1.8, 15.32, <span class="s1">'N'</span><span class="o">)</span>, <span class="o">(</span>2, 1000372, 2.5, 22.15, <span class="s1">'N'</span><span class="o">)</span>, <span class="o">(</span>2, 1000373, 0.9, 9.01, <span class="s1">'N'</span><span class="o">)</span>, <span class="o">(</span>1, 1000374, 8.4, 42.13, <span class="s1">'Y'</span><span class="o">)</span><span class="p">;</span>
</code></pre></div></div>

<p>데이터 조회</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-sql&gt; SELECT <span class="k">*</span> FROM demo.nyc.taxis<span class="p">;</span>
1 1000371 1.8 15.32 N
1 1000374 8.4 42.13 Y
2 1000372 2.5 22.15 N
2 1000373 0.9 9.01 N
Time taken: 1.804 seconds, Fetched 4 row<span class="o">(</span>s<span class="o">)</span>
</code></pre></div></div>

<h2 id="참조">참조</h2>
<p><a href="https://medium.com/@sree_at_work/quickstart-iceberg-with-spark-and-docker-compose-3e7c068720f6">Quickstart Iceberg with Spark and Docker Compose</a></p>]]></content><author><name>Jaeguk Yun</name></author><category term="watson" /><category term="iceberg" /><summary type="html"><![CDATA[Apache Iceberg는 대규모(페타바이트) 분석 데이터 세트를 위한 오픈 테이블 형식(데이터 파일을 구성하는 방법)입니다. 넷플릭스에서 만들었으며 ASF에서 오픈 소스화되었습니다. Netflix, Apple 및 기타 여러 회사에서 광범위하게 사용되고 있습니다. Tabular.io는 Apache Iceberg 테이블을 기반으로 합니다. Dremio.com Arctic은 아파치 아이스버그용으로 구축되었습니다.]]></summary></entry><entry><title type="html">minio docker-compose standard</title><link href="http://localhost:4000/docker/minio/" rel="alternate" type="text/html" title="minio docker-compose standard" /><published>2023-08-19T00:00:00+09:00</published><updated>2023-08-19T00:00:00+09:00</updated><id>http://localhost:4000/docker/minio</id><content type="html" xml:base="http://localhost:4000/docker/minio/"><![CDATA[<p>minio standard docker-compose</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3'
services:
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    container_name: minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: i@mk0rea8785@
    restart: always
    shm_size: '1gb'
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data:/data
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="docker" /><category term="minio" /><summary type="html"><![CDATA[minio standard docker-compose version: '3' services: minio: image: minio/minio command: server /data --console-address ":9001" container_name: minio environment: MINIO_ROOT_USER: admin MINIO_ROOT_PASSWORD: i@mk0rea8785@ restart: always shm_size: '1gb' ports: - "9000:9000" - "9001:9001" volumes: - ./data:/data]]></summary></entry><entry><title type="html">linux listen port 확인</title><link href="http://localhost:4000/docker/linux-port/" rel="alternate" type="text/html" title="linux listen port 확인" /><published>2023-08-18T00:00:00+09:00</published><updated>2023-08-18T00:00:00+09:00</updated><id>http://localhost:4000/docker/linux-port</id><content type="html" xml:base="http://localhost:4000/docker/linux-port/"><![CDATA[<p>1 방법 1: netstat</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@zetawiki:~# netstat <span class="nt">-tnlp</span>
Active Internet connections <span class="o">(</span>only servers<span class="o">)</span>
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:8005          0.0.0.0:<span class="k">*</span>               LISTEN      1212/java       
tcp        0      0 0.0.0.0:3306            0.0.0.0:<span class="k">*</span>               LISTEN      1118/mysqld     
tcp        0      0 0.0.0.0:3690            0.0.0.0:<span class="k">*</span>               LISTEN      919/svnserve    
tcp        0      0 0.0.0.0:8080            0.0.0.0:<span class="k">*</span>               LISTEN      1212/java       
tcp        0      0 0.0.0.0:80              0.0.0.0:<span class="k">*</span>               LISTEN      1157/apache2    
tcp        0      0 0.0.0.0:22              0.0.0.0:<span class="k">*</span>               LISTEN      968/sshd
</code></pre></div></div>

<p>2 방법 2: lsof<br />
lsof가 설치되어 있지 않는 경우</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo yum install -y lsof
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@zetawiki:~# lsof <span class="nt">-i</span> <span class="nt">-nP</span> | <span class="nb">grep </span>LISTEN | <span class="nb">awk</span> <span class="s1">'{print $(NF-1)" "$1}'</span> | <span class="nb">sort</span> <span class="nt">-u</span>
127.0.0.1:8005 java
<span class="k">*</span>:22 sshd
<span class="k">*</span>:3306 mysqld
<span class="k">*</span>:3690 svnserve
<span class="k">*</span>:8080 java
<span class="k">*</span>:80 apache2
</code></pre></div></div>]]></content><author><name>Jaeguk Yun</name></author><category term="docker" /><category term="port" /><summary type="html"><![CDATA[1 방법 1: netstat]]></summary></entry><entry><title type="html">Iceberg와 MinIO를 사용한 레이크하우스 아키텍처</title><link href="http://localhost:4000/watson/iceberg/" rel="alternate" type="text/html" title="Iceberg와 MinIO를 사용한 레이크하우스 아키텍처" /><published>2023-08-18T00:00:00+09:00</published><updated>2023-08-18T00:00:00+09:00</updated><id>http://localhost:4000/watson/iceberg</id><content type="html" xml:base="http://localhost:4000/watson/iceberg/"><![CDATA[<h2 id="iceberg">iceberg</h2>
<p>apache iceberg는 데이터 세계를 강타한 것 같습니다. 처음에 Ryan Blue가 Netflix에서 인큐베이팅한 이 프로젝트는 결국 아파치 소프트웨어 재단으로 옮겨져 현재에 이르고 있습니다. 그 핵심은 대규모 분석 데이터 세트(수백 TB에서 수백 PB에 이르는)를 위한 오픈 테이블 형식입니다.</p>

<p>여러 엔진과 호환되는 형식입니다. 즉, Spark, Trino, Flink, Presto, Hive, Impala가 모두 데이터 집합에서 독립적으로 동시에 작동할 수 있다는 뜻입니다.   데이터 분석의 공용어인 SQL은 물론, 전체 스키마 진화, 숨겨진 파티셔닝, 시간 이동, 롤백 및 데이터 압축과 같은 주요 기능도 지원합니다.</p>

<p>이 게시물에서는 Iceberg와 MinIO가 어떻게 서로를 보완하는지, 그리고 다양한 분석 프레임워크(Spark, Flink, Trino, Dremio, Snowflake)가 이 두 가지를 어떻게 활용할 수 있는지에 대해 중점적으로 설명합니다.</p>

<h2 id="배경">배경</h2>
<p>아파치 하이브는 당시에는 큰 진전을 이루었지만, 분석 애플리케이션이 점점 더 많아지고 다양해지고 정교해짐에 따라 궁극적으로 균열이 보이기 시작했습니다.<br />
성능을 달성하기 위해서는 데이터가 디렉터리에 남아 있어야 하고 이러한 디렉터리를 지속적으로 관리해야 했습니다.</p>

<p>그 결과 디렉터리 데이터베이스를 구축하게 되었습니다.</p>

<p>이렇게 하면 데이터가 어디에 있느냐는 문제는 해결되었지만, 해당 테이블의 상태가 두 곳(디렉터리 데이터베이스와 파일 시스템)에 존재하게 되는 문제가 발생했습니다.</p>

<p>이로 인해 할 수 있는 작업과 유연성이 제한되었으며, 특히 변경 사항과 관련하여 한 번의 작업으로 두 곳 모두에서 보장할 수 없는 변경 사항이 발생했습니다.</p>

<p>다년간에 걸친 대량의 데이터가 날짜별로 분할되어 있다고 상상해 보세요. 연도가 월과 주 단위로 분할되고, 주 단위가 일 단위로 분할되고, 일 단위가 시간 단위로 분할되는 식으로 분할되면 디렉토리 목록이 폭발적으로 증가합니다. Hive 메타스토어(HMS)는 트랜잭션 RDBMS입니다. 파일 시스템(HDFS)은 비트랜잭션입니다. 파티션 정보가 변경되면 파티션 저장소와 파일 시스템을 모두 다시 만들어야 합니다.</p>

<p>이 문제는 지속 가능하지 않았고 아무리 패치를 적용해도 내재된 문제를 해결할 수 없었습니다. 실제로 데이터 증가에 따라 문제는 더욱 가속화되고 있었습니다.</p>

<h2 id="최신-오픈-테이블-형식의-목표">최신 오픈 테이블 형식의 목표</h2>
<p>데이터 레이크하우스 아키텍처의 핵심 판매 포인트 중 하나는 여러 분석 엔진과 프레임워크를 지원한다는 점입니다. 예를 들어, ELT(추출, 로드, 변환)와 ETL(추출, 변환, 로드)을 모두 지원해야 합니다.  비즈니스 인텔리전스, 비즈니스 분석, AI/ML 유형의 워크로드를 지원해야 합니다. 안전하고 예측 가능한 방식으로 동일한 테이블 세트와 성공적으로 인터페이스해야 합니다. 즉, Spark, Flink, Trino, Arrow, Dask와 같은 여러 엔진이 모두 어떤 식으로든 일관된 아키텍처로 연결되어야 합니다.</p>

<p>각 엔진이 성공할 수 있도록 지원하면서 데이터를 효율적으로 저장하는 다중 엔진 플랫폼은 분석 업계가 갈망해 온 것이며, Iceberg 및 Data Lakehouse 아키텍처가 제공하는 것입니다.</p>

<p>데이터를 안정적으로 업데이트하면서 여러 엔진을 사용하는 것은 쉬운 일이 아니며 많은 어려움이 있습니다. 하지만 신뢰할 수 있는 업데이트를 제공하는 두세 가지 형식이 있는 지금도 여전히 많은 혼란이 있고 이 영역에 문제가 있습니다.</p>

<p><img src="/assets/images/watson/data-lake-house.png" alt="" /></p>

<h2 id="apache-iceberg-to-the-rescue">Apache Iceberg to the Rescue</h2>

<p>Apache Iceberg는 처음부터 오픈 테이블 형식을 구현하기 위해 위에서 언급한 대부분의 과제와 목표를 기본으로 설계되었습니다.</p>

<p>다음과 같은 과제를 해결합니다:</p>

<ol>
  <li>유연한 컴퓨팅
    <blockquote>
      <p>데이터를 이동하지 않고 여러 엔진이 원활하게 작동해야 합니다.<br />
배치, 스트리밍, 애드혹 작업 지원<br />
JVM 프레임워크뿐 아니라 다양한 언어의 코드 지원</p>
    </blockquote>
  </li>
  <li>SQL 웨어하우스 동작
    <blockquote>
      <p>CRUD 작업을 안정적으로 수행할 수 있는 SQL 테이블을 사용한 트랜잭션의 안정적 수행<br />
실제 테이블에서 우려 사항을 분리하여 분리된 정보 제공</p>
    </blockquote>
  </li>
</ol>

<p>Apache Hive와 달리 Apache Iceberg는 오브젝트 스토리지에 레코드를 보관합니다. Iceberg를 사용하면 여러 엔진에서 SQL 동작을 활용할 수 있으며, 대규모 테이블을 위해 설계되었습니다. 단일 테이블에 수십 페타바이트의 데이터가 포함될 수 있는 프로덕션 환경에서는 이 점이 매우 중요합니다. 테이블 메타데이터를 선별하기 위해 분산된 SQL 엔진을 사용할 필요 없이 단일 노드에서 멀티 페타바이트 테이블도 읽을 수 있습니다.</p>

<p><img src="img/2023-08-19-23-03-14.png" alt="" />
<img src="/assets/images/watson/02-iceberg-arch.png" alt="" /></p>

<p><a href="https://iceberg.apache.org/spec/">Source: https://iceberg.apache.org/spec/</a></p>

<h2 id="iceberg-fileio-이해">Iceberg FileIO 이해</h2>

<p>FileIO는 핵심 Iceberg 라이브러리와 기본 스토리지 사이의 인터페이스입니다. FileIO는 분산 컴퓨팅과 스토리지가 분리되어 있는 세상에서 Iceberg가 작동하기 위한 방법으로 만들어졌습니다. 레거시 Hadoop 에코시스템은 계층적 경로 지정과 파티션 구조를 필요로 하는데, 이는 실제로는 객체 스토리지 세계에서 속도와 확장성을 달성하는 데 사용되는 방법과는 정반대입니다.</p>

<p>Hadoop과 Hive는 고성능의 확장 가능한 클라우드 네이티브 오브젝트 스토리지에 대한 반(反)패턴입니다. S3 API를 사용해 MinIO와 상호 작용하는 데이터 레이크 애플리케이션은 수백만 또는 수십억 개의 개체에 대해 초당 수천 개의 트랜잭션으로 쉽게 확장할 수 있습니다. 여러 개의 동시 요청을 병렬로 처리하여 읽기 및 쓰기 성능을 향상시킬 수 있습니다. 접두사(첫 번째 문자로 시작하는 개체 이름의 하위 집합인 문자열)를 버킷에 추가한 다음 병렬 작업을 작성하여 각각 접두사당 연결을 열면 됩니다.</p>

<p>이와는 대조적으로, Iceberg는 오브젝트 스토리지를 사용하여 물리적 스토리지로부터 완전히 추상화되어 실행되도록 설계되었습니다. 모든 위치는 메타데이터에 정의된 대로 “명시적이고, 불변하며, 절대적”입니다. Iceberg는 참조 디렉터리의 번거로움 없이 테이블의 전체 상태를 추적합니다. 메타데이터를 사용해 테이블을 찾는 것이 S3 API를 사용해 전체 계층 구조를 나열하는 것보다 훨씬 빠릅니다. 커밋은 메타데이터 테이블에 새 항목을 추가하기만 하면 되므로 이름을 바꿀 필요가 없습니다.</p>

<p>FileIO API는 계획 및 커밋 단계에서 메타데이터 작업을 수행합니다. 작업은 FileIO를 사용하여 기본 데이터 파일을 읽고 쓰며, 이러한 파일의 위치는 커밋 중에 테이블 메타데이터에 포함됩니다. 엔진이 이 작업을 수행하는 정확한 방법은 FileIO의 구현에 따라 다릅니다. 레거시 환경의 경우, HadoopFileIO는 기존 Hadoop 파일 시스템 구현과 Iceberg 내의 FileIO API 사이의 어댑터 계층 역할을 합니다.</p>

<p>여기서는 네이티브 S3 구현인 S3FileIO에 초점을 맞추겠습니다. 클라우드 네이티브 레이크하우스를 구축할 때 Hadoop 크루프트를 가지고 다닐 필요가 없습니다.  <a href="https://tabular.io/blog/iceberg-fileio/?ref=blog.min.io">Iceberg FileIO: 클라우드 네이티브 테이블</a>에 따르면, 네이티브 S3 구현의 장점은 다음과 같습니다:</p>

<p><strong>컨트랙트 동작</strong>: Hadoop 파일 시스템 구현은 엄격한 컨트랙트 동작으로 인해 추가 요청(존재 확인, 디렉터리 및 경로 충돌 해제)이 발생하여 오버헤드와 복잡성이 추가됩니다. Iceberg는 완전히 주소 지정이 가능한 고유 경로를 사용하므로 추가적인 복잡성을 피할 수 있습니다.</p>

<p><strong>최적화된 업로드</strong>: S3FileIO는 데이터를 점진적으로 업로드하여 대용량 작업의 디스크 소비를 최소화하고 출력을 위해 여러 파일이 열려 있을 때 낮은 메모리 소비를 유지함으로써 스토리지/메모리에 최적화합니다.</p>

<p><strong>S3 클라이언트 사용자 지정</strong>: 클라이언트는 최신 주요 AWS SDK 버전(v2)을 사용하며, 사용자가 S3(모든 S3 API 호환 엔드포인트 포함)에서 사용할 수 있도록 클라이언트를 완전히 사용자 지정할 수 있습니다.</p>

<p><strong>직렬화 성능</strong>: HadoopFileIO로 작업을 처리하려면 Hadoop 구성의 직렬화가 필요한데, 이 직렬화는 상당히 방대하며, 성능이 저하된 경우 처리 속도가 느려지고 처리되는 데이터보다 더 많은 오버헤드를 초래할 수 있습니다.</p>

<p><strong>종속성 감소</strong>: Hadoop 파일 시스템 구현은 대규모 종속성 트리를 도입하고 구현을 간소화하여 전반적인 패키징 복잡성을 줄입니다.</p>

<p>Iceberg는 0.11.0 이후의 모든 버전에 대해 Spark 및 Flink 런타임과 함께 번들로 제공되는 iceberg-aws 모듈을 통해 다양한 AWS 서비스와의 통합을 제공합니다.<br />
Iceberg를 사용하면 S3FileIO를 통해 S3에 데이터를 쓸 수 있습니다. S3FileIO를 사용할 때 카탈로그는 io-impl 카탈로그 속성을 사용하여 S3 API를 사용하도록 구성됩니다. S3FileIO는 최적화된 보안(S3 액세스 제어 목록, 세 가지 S3 서버 측 암호화 모드) 및 성능(프로그레시브 멀티파트 업로드)을 위해 최신 S3 기능을 채택하고 있으므로 개체 스토리지 사용 사례에 권장됩니다.</p>

<h2 id="iceberg-and-minio-tutorial">Iceberg and MinIO Tutorial</h2>

<p>현재 Spark는 Iceberg 작업을 위한 가장 풍부한 기능을 갖춘 컴퓨팅 엔진이므로, 이 튜토리얼에서는 Spark와 Spark-SQL을 사용하여 Iceberg의 개념과 기능을 이해하는 데 중점을 둡니다. redhat 8.x에서 Java, 카탈로그 또는 메타데이터 포인터로 PostgreSQL, Spark 및 MinIO를 설치하고 구성하는 동시에 Java 종속성을 신중하게 다운로드하고 구성합니다. 그런 다음 Spark-SQL을 실행하여 테이블을 생성, 채우기, 쿼리 및 수정합니다. 또한 스키마 진화, 숨겨진 파티션 작업, 시간 여행 및 롤백 등 Iceberg로 할 수 있는 몇 가지 멋진 기능도 살펴봅니다. 각 단계가 끝나면 MinIO에 있는 Iceberg 버킷의 스크린샷이 포함되어 있어 백그라운드에서 어떤 일이 벌어지고 있는지 확인할 수 있습니다.</p>

<h2 id="prerequisites">Prerequisites</h2>
<p>minio standalone docker-compose를 이용하여 minio를 설치합니다.</p>

<p>data 폴더생성</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>data
</code></pre></div></div>

<pre><code class="language-YAML">version: '3'
services:
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    container_name: minio
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio1234@
    restart: always
    shm_size: '1gb'
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data:/data
</code></pre>
<p>minio 설치</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up <span class="nt">-d</span>
</code></pre></div></div>

<p>웹브라우저를 통해 http:/myserver:9000 또는 자신의 pc인 경우 http://localhost:9000 으로 접속하여 console login을 확인합니다.
minio consle에서 Access Keys 메뉴에서 access key를 생성합니다. 
<strong>Access Keys &gt; Create access key</strong></p>

<p><img src="/assets/images/watson/03-access-key.png" alt="" /></p>

<p>MinIO 클라이언트를 사용하여 별칭을 설정하고 Iceberg용 버킷을 생성하세요.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> minio bash
mc <span class="nb">alias set </span>minio http://myserver:9000 myaccess-key my-secret-key

Added <span class="sb">`</span>minio<span class="sb">`</span> successfully.

mc mb minio/iceberg
Bucket created successfully <span class="sb">`</span>minio/iceberg<span class="sb">`</span><span class="nb">.</span>
</code></pre></div></div>

<p>Hadoop, AWS S3, JDBC와 같은 다양한 기능을 사용하려면 필요한 JAR(Java 아카이브)을 사용하도록 Spark를 다운로드하고 구성해야 합니다. 또한 필요한 각 JAR과 구성 파일의 올바른 버전이 PATH 및 CLASSPATH에 있어야 합니다.</p>

<p>RHEL 8에 OpenJDK를 설치하려면 먼저 그림과 같이 dnf 명령을 사용하여 시스템 패키지를 업데이트합니다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># dnf update</span>
그런 다음 다음 명령을 사용하여 OpenJDK 8 및 11을 설치합니다.

<span class="c"># dnf install java-1.8.0-openjdk-devel  	#install JDK 8</span>
<span class="c"># dnf install java-11-openjdk-devel		#install JDK 11</span>
설치 프로세스가 완료되면 다음 명령을 사용하여 설치된 Java 버전을 확인할 수 있습니다.

<span class="c"># java -version</span>
</code></pre></div></div>

<p>## PostgreSQL 구성하기</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: <span class="s1">'3.5'</span>

services:
  postgres:
    container_name: postgres
    image: postgres:latest
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: admin1234
      PGDATA: /data/postgres
    volumes:
       - postgres-db:/data/postgres
    ports:
      - <span class="s2">"5432:5432"</span>

volumes:
  postgres-db:
    driver: <span class="nb">local</span>
</code></pre></div></div>

<p>postgres 설치</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up <span class="nt">-d</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> postgres bash
pgsql <span class="nt">-U</span> postgre
create user icebergcat with PASSWORD <span class="s1">'minio'</span> CREATEDB<span class="p">;</span>
create database icebergcat owner icebergcat ENCODING <span class="s1">'UTF-8'</span><span class="p">;</span>
ALTER USER icebergcat WITH SUPERUSER<span class="p">;</span>
CREATE SCHEMA icebergcat<span class="p">;</span>
<span class="se">\q</span>

psql <span class="nt">-U</span> icebergcat <span class="nt">-d</span> icebergcat <span class="nt">-W</span> <span class="nt">-h</span> 127.0.0.1
<span class="se">\q</span>

</code></pre></div></div>

<p>Download, extract, and move Apache Spark</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>wget https://dlcdn.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz
<span class="nv">$ </span><span class="nb">tar </span>zxvf spark-3.2.4-bin-hadoop3.2.tgz
<span class="nv">$ </span><span class="nb">sudo mv </span>spark-3.2.4-bin-hadoop3.2/ /opt/spark 
</code></pre></div></div>

<p>.bashrc에 다음을 추가하고 셸을 다시 시작하여 변경 사항을 적용하여 Spark 환경을 설정합니다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/opt/spark
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$SPARK_HOME</span>/bin:<span class="nv">$SPARK_HOME</span>/sbin
bash <span class="nt">-l</span>
</code></pre></div></div>

<p>다음 .jar 파일이 필요합니다. .jar 파일을 다운로드하여 스파크 머신의 필요한 위치(예: /opt/spark/jars)에 복사합니다.</p>

<p>S3 프로토콜을 지원하려면 aws-java-sdk-bundle/1.11.901.jar(또는 그 이상)이 필요합니다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.17.230/bundle-2.17.230.jar
<span class="nb">mv </span>bundle-2.17.230.jar /opt/spark/jars/
</code></pre></div></div>

<p>Download iceberg-spark-runtime-3.2_2.12.jar .</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.1_2.12/0.13.2/iceberg-spark-runtime-3.1_2.12-0.13.2.jar

<span class="nb">mv </span>iceberg-spark-runtime-3.1_2.12-0.13.2.jar /opt/spark/jars/
</code></pre></div></div>

<h2 id="start-spark">Start Spark</h2>
<p>Spark 독립 실행형 마스터 서버 시작</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># spark-master : /etc/hosts</span>
<span class="nv">MASTER</span><span class="o">=</span>spark://spark-master:7077
start-master.sh
<span class="c"># Start a Spark worker process</span>
start-slave.sh <span class="k">${</span><span class="nv">MASTER</span><span class="k">}</span>
</code></pre></div></div>

<p><img src="/assets/images/watson/04-sparksql.png" alt="" /></p>

<p>Spark is alive at spark://<Your-Machine-Name>:7077</Your-Machine-Name></p>

<p>Start a Spark worker process</p>

<h2 id="spark-sql-and-iceberg">Spark-SQL and Iceberg</h2>

<p>이 구성에 대한 몇 가지 중요한 참고 사항</p>

<ul>
  <li>JDBC를 사용하여 내부 IP 주소에서 PostgreSQL에 연결하고 메타데이터에 icebergcat 테이블을 사용하는 카탈로그 my_catalog를 선언합니다.</li>
  <li>그런 다음 웨어하우스 위치를 앞서 생성한 MinIO 버킷으로 설정하고 S3FileIO를 사용하여 액세스하도록 Iceberg를 구성했습니다.</li>
</ul>

<h2 id="start-sparksql">start-sparksql</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># start-env.sh</span>
<span class="nv">DEPENDENCIES</span><span class="o">=</span><span class="s1">''</span>
<span class="nb">export </span><span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span>flsfYX3esSeyCrOAm45k
<span class="nb">export </span><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span>PCoVFDxrVIt1zAwzNIgZAOklsrnlF4q85ZA2uJ9w
<span class="nb">export </span><span class="nv">AWS_S3_ENDPOINT</span><span class="o">=</span>119.81.34.106:9000
<span class="nb">export </span><span class="nv">AWS_REGION</span><span class="o">=</span>us-east-1
<span class="nb">export </span><span class="nv">MINIO_REGION</span><span class="o">=</span>us-east-1
<span class="nb">export </span><span class="nv">DEPENDENCIES</span><span class="o">=</span><span class="s2">"org.apache.iceberg:iceberg-spark-runtime-3.2_2.13:1.3.1"</span>
<span class="nb">export </span><span class="nv">AWS_SDK_VERSION</span><span class="o">=</span>2.17.230
<span class="nb">export </span><span class="nv">AWS_MAVEN_GROUP</span><span class="o">=</span>software.amazon.awssdk
<span class="nb">export </span><span class="nv">AWS_PACKAGES</span><span class="o">=(</span>
<span class="s2">"bundle"</span>
<span class="s2">"url-connection-client"</span>
<span class="o">)</span>
<span class="k">for </span>pkg <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_PACKAGES</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
</span><span class="nb">echo</span> <span class="nv">$pkg</span>
<span class="nb">export </span>DEPENDENCIES+<span class="o">=</span><span class="s2">",</span><span class="nv">$AWS_MAVEN_GROUP</span><span class="s2">:</span><span class="nv">$pkg</span><span class="s2">:</span><span class="nv">$AWS_SDK_VERSION</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># start-sparksql.sh</span>
spark-sql <span class="nt">--packages</span> <span class="nv">$DEPENDENCIES</span> <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.cli.print.header<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog<span class="o">=</span>org.apache.iceberg.spark.SparkCatalog <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.catalog-impl<span class="o">=</span>org.apache.iceberg.jdbc.JdbcCatalog <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.jdbc.verifyServerCertificate<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.uri<span class="o">=</span>jdbc:postgresql://119.81.34.106:5432/icebergcat <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.jdbc.user<span class="o">=</span>icebergcat <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.jdbc.password<span class="o">=</span>minio <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.warehouse<span class="o">=</span>s3://iceberg <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.io-impl<span class="o">=</span>org.apache.iceberg.aws.s3.S3FileIO <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.my_catalog.s3.endpoint<span class="o">=</span>http://127.0.0.1:9000 <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalog.sparkcatalog<span class="o">=</span>org.apache.iceberg.spark.SparkSessionCatalog <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.defaultCatalog<span class="o">=</span>my_catalog <span class="se">\</span>
<span class="nt">--conf</span> spark.eventLog.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
<span class="nt">--conf</span> spark.eventLog.dir<span class="o">=</span>/home/nexweb/iceicedata/spark-events <span class="se">\</span>
<span class="nt">--conf</span> spark.history.fs.logDirectory<span class="o">=</span> /home/nexweb/iceicedata/spark-events <span class="se">\</span>
<span class="nt">--conf</span> spark.sql.catalogImplementation<span class="o">=</span><span class="k">in</span><span class="nt">-memory</span>
</code></pre></div></div>

<h2 id="creating-a-table">Creating a Table</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE my_catalog.my_table <span class="o">(</span>
<span class="nb">id </span>bigint,
data string,
category string<span class="o">)</span>
USING iceberg
LOCATION <span class="s1">'s3://iceberg'</span>
PARTITIONED BY <span class="o">(</span>category<span class="o">)</span><span class="p">;</span>
</code></pre></div></div>

<p>다음은 Iceberg가 S3FileIO를 통해 제공하는 엄청난 성능 향상입니다. 기존 Hive 스토리지 레이아웃을 S3와 함께 사용할 때 객체 접두사에 따라 요청을 스로틀링하는 바람에 성능이 느려져 어려움을 겪었던 사용자들에게 큰 도움이 될 것입니다. AWS S3에서 파티셔닝된 Athena/Hive 테이블을 생성하는 데 30~60분이 소요된다는 것은 잘 알려진 사실입니다. Iceberg는 기본적으로 Hive 스토리지 레이아웃을 사용하지만, ObjectStoreLocationProvider를 사용하도록 전환할 수 있습니다. ObjectStoreLocationProvider를 사용하면 저장된 각 파일에 대해 결정론적 해시가 생성되며, 이 해시는 write.data.경로 바로 뒤에 추가됩니다. 이렇게 하면 S3 호환 오브젝트 스토리지에 쓰여진 파일이 S3 버킷의 여러 접두사에 균등하게 분산되어 S3 관련 IO 작업의 스로틀링을 최소화하고 처리량을 최대화할 수 있습니다. ObjectStoreLocationProvider를 사용할 때, Iceberg 테이블 전체에 걸쳐 공유되고 짧은 write.data.경로를 사용하면 성능이 향상됩니다. 이 외에도 Iceberg에서는 Hive에 비해 성능과 안정성을 개선하기 위해 훨씬 더 많은 작업이 수행되었습니다.</p>

<p>다음으로, 모의 데이터를 삽입하고 Iceberg가 MinIO에 저장하는 파일을 살펴보겠습니다. 이제 Iceberg 버킷 내부에 my_table/metadata 및 my_table/data 접두사가 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INSERT INTO my_catalog.my_table VALUES <span class="o">(</span>1, <span class="s1">'a'</span>, <span class="s2">"music"</span><span class="o">)</span>, <span class="o">(</span>2, <span class="s1">'b'</span>, <span class="s2">"music"</span><span class="o">)</span>, <span class="o">(</span>3, <span class="s1">'c'</span>, <span class="s2">"video"</span><span class="o">)</span><span class="p">;</span>
</code></pre></div></div>

<h2 id="query-실행">Query 실행</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-sql&gt; SELECT count<span class="o">(</span>1<span class="o">)</span> as count, data
FROM my_catalog.my_table
GROUP BY data<span class="p">;</span>
1       a
1       b
1       c
Time taken: 9.715 seconds, Fetched 3 row<span class="o">(</span>s<span class="o">)</span>
spark-sql&gt;
</code></pre></div></div>
<h2 id="aws-cli-설치">aws-cli 설치</h2>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install </span>awscli <span class="nt">--upgrade</span> <span class="nt">--user</span>

</code></pre></div></div>
<p>Begin AWS CLI configuration</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws configure
</code></pre></div></div>

<h2 id="참고">참고</h2>
<p><a href="https://github.com/tlepple/iceberg-intro-workshop">iceberg-intro-workshop</a></p>]]></content><author><name>Jaeguk Yun</name></author><category term="watson" /><category term="iceberg" /><summary type="html"><![CDATA[iceberg apache iceberg는 데이터 세계를 강타한 것 같습니다. 처음에 Ryan Blue가 Netflix에서 인큐베이팅한 이 프로젝트는 결국 아파치 소프트웨어 재단으로 옮겨져 현재에 이르고 있습니다. 그 핵심은 대규모 분석 데이터 세트(수백 TB에서 수백 PB에 이르는)를 위한 오픈 테이블 형식입니다.]]></summary></entry><entry><title type="html">LLM 앱을 구축하기 위해 알아야 할 모든 것</title><link href="http://localhost:4000/watson/my-first-llm-app/" rel="alternate" type="text/html" title="LLM 앱을 구축하기 위해 알아야 할 모든 것" /><published>2023-07-28T00:00:00+09:00</published><updated>2023-07-28T00:00:00+09:00</updated><id>http://localhost:4000/watson/my-first-llm-app</id><content type="html" xml:base="http://localhost:4000/watson/my-first-llm-app/"><![CDATA[<h2 id="llm이-필요한-이유">LLM이 필요한 이유</h2>
<p>언어의 진화는 오늘날까지 인간을 엄청나게 발전시켰습니다. 덕분에 오늘날 우리가 알고 있는 형태로 지식을 효율적으로 공유하고 협업할 수 있게 되었습니다. 그 결과, 우리의 집단 지식은 대부분 정리되지 않은 서면 텍스트를 통해 계속 보존되고 전달되고 있습니다.</p>

<p>지난 20년 동안 정보와 프로세스를 디지털화하기 위해 수행된 이니셔티브는 종종 관계형 데이터베이스에 점점 더 많은 데이터를 축적하는 데 중점을 두었습니다. 이러한 접근 방식은 기존의 분석 머신러닝 알고리즘이 데이터를 처리하고 이해할 수 있게 해줍니다.</p>

<p>그러나 점점 더 많은 양의 데이터를 구조화된 방식으로 저장하기 위한 광범위한 노력에도 불구하고 여전히 지식의 전체를 포착하고 처리할 수는 없습니다.</p>

<blockquote>
  <p>기업 내 모든 데이터의 약 80%는<br />
업무 설명, 이력서, 이메일, 텍스트 문서, 파워포인트 슬라이드, 음성 녹음, 동영상 및 소셜 미디어와 같은<br />
비정형 데이터입니다.</p>

</blockquote>

<p>GPT3.5로 이어지는 개발과 발전은 구조의 유무에 관계없이<br />
 다양한 데이터 세트를 효과적으로 해석하고 분석할 수 있도록 지원한다는 점에서 중요한 이정표가 될 것입니다.<br />
오늘날에는 텍스트, 이미지, 오디오 파일 등 다양한 형태의 콘텐츠를 이해하고<br />
생성할 수 있는 모델을 보유하고 있습니다.</p>]]></content><author><name>Jaeguk Yun</name></author><category term="watson" /><category term="LLM" /><summary type="html"><![CDATA[LLM이 필요한 이유 언어의 진화는 오늘날까지 인간을 엄청나게 발전시켰습니다. 덕분에 오늘날 우리가 알고 있는 형태로 지식을 효율적으로 공유하고 협업할 수 있게 되었습니다. 그 결과, 우리의 집단 지식은 대부분 정리되지 않은 서면 텍스트를 통해 계속 보존되고 전달되고 있습니다.]]></summary></entry><entry><title type="html">코딩없이 ChatBot 만들기</title><link href="http://localhost:4000/watson/watson-assistant-tutorial/" rel="alternate" type="text/html" title="코딩없이 ChatBot 만들기" /><published>2023-07-27T00:00:00+09:00</published><updated>2023-07-27T00:00:00+09:00</updated><id>http://localhost:4000/watson/watson-assistant-tutorial</id><content type="html" xml:base="http://localhost:4000/watson/watson-assistant-tutorial/"><![CDATA[<h2 id="ibm-왓슨-어시스턴트로-챗봇을-구축하는-방법">IBM 왓슨 어시스턴트로 챗봇을 구축하는 방법</h2>
<p>이 글에서는 IBM 왓슨 어시스턴트를 사용하여 작동하는 챗봇을 구축하는 방법에 대해 알아보겠습니다.<br />
왓슨 어시스턴트는 IBM 클라우드에서 호스팅되는 챗봇 구축용 서비스로, 별도의 프로그래밍 없이도 챗봇을 구축할 수 있습니다.</p>

<p><img src="/assets/images/watson/01-bot.png" alt="" /></p>

<p>챗봇 구축을 시작하기 전에 챗봇이란 무엇이며 챗봇의 사용 사례를 이해해 보겠습니다.</p>
<blockquote>
  <p>챗봇은 사람과 대화를 시뮬레이션할 수 있는 인공 지능(AI) 소프트웨어입니다. 챗봇 서비스는 사용자의 텍스트, 
시각 또는 청각 입력에 응답하도록 프로그래밍되어 있으며 SMS, 웹사이트 채팅창, 소셜 메시징 서비스 등의 매체를 활용하여 메시지를 수신하고 응답합니다.</p>
</blockquote>

<p>챗봇은 FAQ 답변, 24*7 고객 관리 지원, 티켓 예약, 피자 주문 접수 등 다양한 서비스에서 사용할 수 있습니다.</p>

<p>다음 작업을 처리할 수 있는 레스토랑용 챗봇을 구축하겠습니다:</p>

<ol>
  <li><strong>예약하기</strong> - 챗봇이 최종 사용자를 위해 테이블을 예약할 수 있습니다.</li>
  <li><strong>영업시간</strong> - 챗봇이 레스토랑의 영업 시간에 대한 정보를 제공할 수 있습니다.</li>
</ol>

<p>원하는 만큼 많은 기능을 추가할 수 있지만,<br />
단순화와 이해를 돕기 위해 위의 두 가지 기능에만 초점을 맞춰 설명하겠습니다.</p>

<p>이제 챗봇이 수행할 수 있는 작업의 종류에 대한 의도를 설정했으니 구축을 시작하겠습니다.</p>

<h2 id="챗봇-만들기">챗봇 만들기</h2>
<p>IBM 왓슨 어시스턴트 서비스를 사용하려면 IBM 클라우드 계정이 있어야 합니다.<br />
IBM Cloud 계정에 로그인합니다. 계정이 없는 경우 여기에서 무료로 가입할 수 있습니다.</p>

<p><a href="https://cloud.ibm.com/registration">IBM Cloud 가입</a></p>

<p><strong>1단계: 왓슨 어시스턴트 서비스 인스턴스 만들기</strong></p>

<p>IBM 클라우드 계정에 로그인하면 대시보드에 아래와 같은 화면이 나타납니다.<br />
<img src="/assets/images/watson/02-dashboard.png" alt="" /></p>

<p>검색창에 “왓슨 어시스턴트”를 입력하고 왓슨 어시스턴트 서비스를 생성합니다.</p>

<p>왓슨 어시스턴트 서비스를 생성한 후 해당 서비스를 클릭하면 다음과 같은 화면이 나타납니다.</p>

<p><img src="/assets/images/watson/03-start-by-launching the tool.png" alt="" /></p>

<p>이제 ‘왓슨 어시스턴트 실행’을 클릭하여 서비스를 시작합니다.</p>

<p><strong>2단계: 어시스턴트에 대화 스킬 추가하기</strong>
첫 번째 단계를 완료하면 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/04-add-a-dialog-skill-to the assistant.png" alt="" /></p>

<p>이 단계에서는 어시스턴트에 대화 스킬을 추가하겠습니다.<br />
대화 스킬은 어시스턴트가 언급된 작업을 수행하는 데 도움이 됩니다.</p>

<p>어시스턴트에 대화 스킬을 추가하려면 “내 첫 번째 어시스턴트”를 클릭합니다. 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/05-my-first-assistant.png" alt="" /></p>

<p>여기에서 어시스턴트에 연결된 액션 스킬이 있음을 알 수 있습니다.<br />
어떤 경우에는 어시스턴트에 연결된 액션 스킬이 있고 어떤 경우에는 어시스턴트에 연결된 대화 스킬이 있습니다.</p>

<p>어시스턴트에 연결된 대화 스킬이 있는 경우에는 아무것도 변경할 필요가 없지만, 우리의 경우에는<br />
어시스턴트에 연결된 액션 스킬이 있고 대화 스킬을 연결하려고 합니다.<br />
이렇게 하려면 먼저 어시스턴트에서 액션 스킬을 제거합니다.</p>

<p><img src="/assets/images/watson/06-remove-action-skill.png" alt="" /></p>

<p>기존 액션 스킬을 제거하려면 내 첫 번째 스킬의 오른쪽 상단 모서리에 있는 점 3개를 클릭하고 스킬 제거를 클릭합니다.<br />
액션 스킬을 성공적으로 제거하면 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/07-add-dialog-skill.png" alt="" /></p>

<p>이제 대화 스킬을 추가하려면 “액션 또는 대화 스킬 추가”를 클릭하고<br />
“Restaurant Skill” 이라는 이름의 새 대화 스킬을 만든 다음 스킬 만들기를 클릭합니다.</p>

<p><img src="/assets/images/watson//08-add-action-or-dialog.png" alt="" /></p>

<p>이 작업을 완료하면 어시스턴트에 새 대화 스킬이 추가됩니다. 이것으로 챗봇 구축의 2단계가 완료됩니다.</p>

<p><img src="/assets/images/watson/09-complete-step2.png" alt="" /></p>

<p><strong>3단계: 인텐트를 만들고 스킬에 추가하기</strong></p>

<p>이 단계에서는 인텐트를 생성하고 스킬에 추가하겠습니다. 하지만 그 전에 인텐트가 무엇인지 이해해야 합니다.</p>

<p>왓슨 어시스턴트에 따르면</p>

<blockquote>
  <p>인텐트는 동일한 의미를 가진 사용자 질의 문장의 모음입니다.<br />
인텐트를 만들면 어시스턴트가 사용자가 목표를 표현하는 다양한 방식을 이해하도록 훈련시킬 수 있습니다.</p>
</blockquote>

<p>인텐트는 단순히 동일한 의미를 가진 사용자 문장의 모음입니다. 
이러한 문장은 사용자가 원하는 목표를 표현하는 문장이 될 수 있으며, 이 경우 테이블 예약하기와 같은 문장이 될 수 있습니다.</p>

<p>사용자가 레스토랑에서 테이블을 예약하기 위해 말할 수 있는 다양한 방법에는 어떤 것이 있나요?</p>

<p>테이블 예약하기.</p>

<p>나를 위한 테이블 예약하기.</p>

<p>레스토랑의 테이블을 예약하세요.</p>

<p>테이블 예약하기.</p>

<p>저녁 식사를 위한 테이블을 예약하고 싶습니다.</p>

<p>이 모든 예는 사용자가 테이블을 예약하려는 의도를 보여줍니다.<br />
이 예시를 보면 사용자가 테이블을 예약하고 싶다는 것을 쉽게 알 수 있습니다. 챗봇의 인텐트도 마찬가지입니다.</p>

<p>인텐트는 챗봇이 사용자가 달성하고자 하는 목표를 식별하는 데 도움이 됩니다. <br />
이러한 목표는 예약하기, 운영 시간 정보를 얻기 등이 될 수 있습니다.</p>

<p>챗봇에서는 예약을 하고 영업시간에 대한 정보를 얻는 두 가지 주요 목표를 달성하고자 합니다.<br />
따라서 각각에 대해 하나씩 두 개의 인텐트를 만들 것입니다.</p>

<p>먼저, 사용자의 테이블 예약 의도를 파악하기 위해 인텐트를 생성합니다.</p>

<p>인텐트를 생성하려면 먼저 레스토랑 스킬을 클릭합니다. 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/09-make-intent.png" alt="" /></p>

<p>이제 인텐트 생성을 클릭합니다. 그런 다음 인텐트 이름 “make_booking”을 입력한 다음 인텐트 생성을 클릭합니다.<br />
다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/10-make-a-booking.png" alt="" /></p>

<p>이제 인텐트 생성을 클릭합니다. 그런 다음 인텐트 이름 “make_booking”을 입력한 다음 인텐트 생성을 클릭합니다. 다음 이미지가 표시됩니다.</p>

<p>테이블을 예약.</p>

<p>나를 위해 테이블을 예약.</p>

<p>레스토랑의 테이블을 예약.</p>

<p>테이블 예약하기.</p>

<p>저녁 식사를 위한 테이블을 예약하고 싶습니다.</p>

<p>예제를 추가하려면 문장을 입력한 다음 예제 추가를 클릭하세요. 모든 예제를 성공적으로 추가하면 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/11-add-examples.png" alt="" /></p>

<p>마찬가지로 다음 예제를 사용하여 운영 시간에 대한 다른 인텐트를 만듭니다.</p>

<p>운영 시간은 어떻게 되나요?</p>

<p>주말에 영업하나요?</p>

<p>일요일에도 영업하나요?</p>

<p>근무 시간은 어떻게 되나요?</p>

<p>레스토랑의 영업 시간은 언제인가요?</p>

<p><img src="/assets/images/watson/12-house-of-operations.png" alt="" /></p>

<p><strong>결과</strong><br />
<img src="/assets/images/watson/13-house-of-operations-examples.png" alt="" /></p>

<p>지금까지 왓슨 어시스턴트 서비스 인스턴스를 생성하고 어시스턴트에 대화 스킬을 추가하고 두 개의 인텐트를 생성했습니다.<br />
다음 단계로 넘어가겠습니다.</p>

<p><strong>4단계: 엔티티를 생성하고 스킬에 추가하기</strong></p>

<p>이 단계에서는 엔티티를 만들어 스킬에 추가하겠습니다. 하지만 그 전에 엔티티가 무엇이며 챗봇을 구축하는 데 어떻게 사용될 수 있는지 이해해 보겠습니다.</p>

<p>왓슨 어시스턴트에 따르면…</p>

<blockquote>
  <p>엔티티는 사용자 입력에서 사용자의 목적과 관련된 정보를 나타냅니다.</p>
</blockquote>

<p>인텐트가 동사(사용자가 수행하려는 작업)를 나타내는 경우 엔티티는 명사(해당 작업의 대상 또는 해당 작업의 컨텍스트)를 나타냅니다.<br />
예를 들어 일기 예보를 확인하는 것이 인텐트인 경우 애플리케이션이 정확한 예보를 반환하려면 관련 위치 및 날짜 엔티티가 필요합니다.</p>

<p>이 예제에서 사용자는 레스토랑에서 테이블을 예약하려고 합니다.<br />
하지만 테이블을 예약하려면 챗봇은 사용자가 몇 시, 몇 날짜에 테이블을 예약하려는지 알아야 합니다.<br />
여기서 날짜와 시간은 테이블을 예약하는 데 필요한 엔티티입니다.</p>

<p>따라서 대화 스킬에 날짜와 시간이라는 두 개의 엔티티를 추가하겠습니다. 다행히도 이 두 엔티티를 만들 필요는 없습니다.<br />
왓슨 어시스턴트에서 이 두 엔티티는 시스템 엔티티에 미리 정의되어 있습니다.<br />
이 두 엔티티를 사용하도록 설정하기만 하면 됩니다.</p>

<p>이 두 엔티티를 활성화하려면 “Restuarant 스킬”에서 “엔티티”를 클릭합니다.<br />
클릭하면 다음 이미지가 표시됩니다</p>

<p><img src="/assets/images/watson/13-entities.png" alt="" /></p>

<p>엔티티 섹션에 내 엔티티와 시스템 엔티티라는 두 가지 옵션이 표시됩니다.<br />
시스템 엔티티를 클릭하고 시스템 날짜 및 시스템 시간 엔티티를 활성화하면 챗봇에서 사용할 수 있습니다.</p>

<p><img src="/assets/images/watson/14-systems-entities.png" alt="" /></p>

<p>이제 시스템 날짜 및 시스템 시간 엔티티를 활성화했습니다. 이제 챗봇을 구축하는 데 사용할 수 있습니다.</p>

<p><strong>5단계: 대화 상자 만들기</strong> .</p>

<p>이제 사용자 입력에 대한 응답을 제공하는 대화 상자를 만들겠습니다.<br />
하지만 그 전에 대화 상자가 무엇인지 이해해 봅시다.</p>

<p>왓슨 어시스턴트에 따르면…</p>

<blockquote>
  <p>Dialog는 논리 트리의 형태로 대화의 흐름을 정의합니다.<br />
이 트리는 의도(사용자가 말하는 내용)와 응답(챗봇이 응답하는 내용)을 일치시킵니다.<br />
트리의 각 노드에는 사용자 입력에 따라 트리를 트리거하는 조건이 있습니다.</p>
</blockquote>

<p>Dialog를 만들려면 스킬 메뉴에서 ‘Dilaog’를 클릭하면 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/15-create-dailog.png" alt="" /></p>

<p>여기에서 자동으로 생성되는 두 개의 대화 상자 노드를 볼 수 있습니다.</p>

<ol>
  <li><strong>Welcome</strong>: 사용자가 어시스턴트와 처음 상호작용할 때 사용자에게 표시되는 인사말이 포함되어 있습니다.</li>
  <li><strong>기타</strong>: 사용자의 입력이 인식되지 않을 때 사용자에게 응답하는 데 사용되는 문구가 포함되어 있습니다.</li>
</ol>

<p>이제 환영 노드를 클릭하고 어시스턴트 응답 섹션에서 텍스트를 다음과 같이 작성된 텍스트로 바꿉니다. 그리고 질문을 랜덤으로 하기위해 Random을 클릭하고 다음과 같이 텍스트를 입력합니다.</p>

<p>안녕하세요! 저는 레스토랑의 AI 비서입니다. 무엇을 도와드릴까요?
무엇을 도와드릴까요?
<img src="/assets/images/watson/16-how-can-i-help-you.png" alt="" /></p>

<p>사용자가 챗봇과 대화할 때마다 먼저 위에 작성된 메시지를 받게 됩니다.<br />
이제 두 개의 대화 노드가 더 필요합니다. 하나는 운영 시간에 대한 정보를 제공하고 다른 하나는 예약을 할 수 있도록 합니다.</p>

<p>먼저 영업 시간부터 시작하겠습니다.<br />
새 노드를 추가하려면 환영 노드의 오른쪽 상단에 있는 점 3개를 클릭하고 아래의 노드 추가를 클릭합니다. <br />
클릭하면 다음 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/17-create-node.png" alt="" /></p>

<p>이제 새로 생성된 노드에 다음 세부 정보를 입력합니다.</p>

<ol>
  <li>노드 이름(Enter node name)을 입력합니다(선택 사항): 운영 시간</li>
  <li>조건 입력(Enter condition): #hoursOfOperation 입력 후 #운영시간을 클릭합니다.</li>
  <li>응답 텍스트를 입력합니다(Enter response text): 레스토랑 운영 시간은 평일 오전 10시부터 오후 10시, 주말 오전 10시부터 오후 9시까지입니다</li>
</ol>

<p>이제 사용자 지정 버튼 옆의 십자 아이콘을 클릭합니다.</p>

<p><img src="/assets/images/watson/18-customize.png" alt="" /></p>

<p>이제 사용자가 레스토랑의 영업 시간을 문의할 때마다 영업 시간 노드가 트리거되고 다음과 같은 응답이 표시됩니다:</p>

<blockquote>
  <p>레스토랑 운영 시간은 평일 오전 10시부터 오후 10시,<br />
주말 오전 10시부터 오후 9시까지입니다</p>
</blockquote>

<p>이제 저희가 구축한 챗봇을 사용해 보실 수 있습니다. 오른쪽 상단의 Tryit 을 클릭하고 다음 문장을 입력하세요.</p>

<p><img src="/assets/images/watson/19-try-it.png" alt="" /></p>

<p>운영 시간이 어떻게 되나요?</p>

<p>다음과 같은 응답이 표시됩니다.</p>

<p><img src="/assets/images/watson/20-restaurant-response.png" alt="" /></p>

<p>지금까지 영업 시간에 대한 정보를 제공하는 대화 노드를 추가했습니다.<br />
이제 마지막 대화 노드를 추가하여 레스토랑의 테이블을 예약하겠습니다.</p>

<p>새 대화 노드를 추가하려면 영업 시간 노드에서 점 세 개를 클릭하고 위의 추가 노드를 클릭하면 됩니다.</p>

<p>다음과 같은 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/21-add-node-booking.png" alt="" /></p>

<p>이제 새로 생성된 대화 상자 노드에 다음 세부 정보를 입력합니다.</p>

<p>노드 이름을 입력합니다(선택 사항): Make Booking Restaurant
조건 입력: #예약 을 입력합니다.</p>

<p><img src="/assets/images/watson/22-make-booking.png" alt="" /></p>

<p>사용자가 테이블 예약에 대해 문의할 때마다 챗봇이 #예약 인텐트를 인식하고 이 노드가 트리거되도록 인식 조건으로 #예약 인텐트를 입력했습니다.</p>

<p>레스토랑에서 테이블을 예약하려면 챗봇은 사용자에게 몇 가지 질문을 해야 합니다.</p>

<ol>
  <li>지점을 선택해주세요</li>
  <li>날짜는 어느 날짜로 하시겠습니까?</li>
  <li>시간은 몇시로 예약해 드릴까요?</li>
  <li>예약인원은 몇명이세요?</li>
</ol>

<p><img src="/assets/images/watson/22-workflow-booking a table.png" alt="" /></p>

<p>이 워크플로를 만들기 위해 슬롯을 사용하겠습니다. 슬롯은 단일 노드 내에서 사용자에게 여러 가지 정보를 요청하고 저장할 수 있는 구조화된 형식을 제공합니다. 특정 작업을 염두에 두고 있고 그 작업을 수행하기 전에 사용자로부터 핵심 정보가 필요할 때 가장 유용합니다.</p>

<p>슬롯을 사용하려면 예약하기 노드의 오른쪽 상단 모서리에 있는 사용자 지정 버튼을 클릭하고 슬롯 스위치를 켜짐으로 설정한 다음 적용을 클릭합니다.</p>

<p><img src="/assets/images/watson/23-customize make booking.png" alt="" /></p>

<p>에서 섹션을 확인하고 다음 슬롯을 추가합니다:</p>

<p>@sys-date 확인, 다른 이름으로 저장 - $date, 없는 경우 - 몇 시에 오시겠습니까?<br />
@sys-time 확인, 다른 이름으로 저장 - $시간, 없는 경우 - 몇 시에 예약하시겠습니까?<br />
슬롯을 추가하면 다음과 같은 이미지가 표시됩니다.</p>

<p><img src="/assets/images/watson/24-slot.png" alt="" /></p>

<p>이제 어시스턴트 응답 섹션에 다음 텍스트를 입력합니다.</p>

<p>$branch 지점에 $date 에  $time 으로  $guests 명 예약되었습니다.</p>

<p>이제 챗봇이 완성되었습니다.<br />
오른쪽 상단의 Tryit 옵션을 사용하여 사용해 볼 수 있습니다.</p>]]></content><author><name>Jaeguk Yun</name></author><category term="watson" /><category term="watson assistant" /><summary type="html"><![CDATA[IBM 왓슨 어시스턴트로 챗봇을 구축하는 방법 이 글에서는 IBM 왓슨 어시스턴트를 사용하여 작동하는 챗봇을 구축하는 방법에 대해 알아보겠습니다. 왓슨 어시스턴트는 IBM 클라우드에서 호스팅되는 챗봇 구축용 서비스로, 별도의 프로그래밍 없이도 챗봇을 구축할 수 있습니다.]]></summary></entry><entry><title type="html">맥환경에서 eclipse 자동완성 단축키 설정</title><link href="http://localhost:4000/springboot/auto-sts/" rel="alternate" type="text/html" title="맥환경에서 eclipse 자동완성 단축키 설정" /><published>2023-06-01T00:00:00+09:00</published><updated>2023-06-01T00:00:00+09:00</updated><id>http://localhost:4000/springboot/auto-sts</id><content type="html" xml:base="http://localhost:4000/springboot/auto-sts/"><![CDATA[<p>윈도우에서 자동완성 단축키는 Ctrl + Space 입니다. 하지만 맥북환경에서는 Ctrl + Space로 기본으로 설정되어 있습니다.
그래서 자동완성 단축키를 Shift + Space로 변경해서 사용합니다.</p>

<p><img src="/assets/images/springboot/05-sts-short-cut-mac.png" alt="sts-short-cut-mac" /></p>]]></content><author><name>Jaeguk Yun</name></author><category term="springboot" /><category term="short-key" /><summary type="html"><![CDATA[윈도우에서 자동완성 단축키는 Ctrl + Space 입니다. 하지만 맥북환경에서는 Ctrl + Space로 기본으로 설정되어 있습니다. 그래서 자동완성 단축키를 Shift + Space로 변경해서 사용합니다.]]></summary></entry></feed>